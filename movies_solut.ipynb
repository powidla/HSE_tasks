{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>, ¬©<a href=https://www.hse.ru/ target=\"_blank\">HSE</a> 2021 onwards</font></small><hr style=\"margin:0;background-color:silver\">\n",
        "\n",
        "# **[üèÜüéûÔ∏èMovieGenres](https://www.kaggle.com/competitions/oct312022moviegenres/rules)** <small>(See private URL in LMS assignment)</small>\n",
        "\n",
        "See [**instructions**](https://colab.research.google.com/drive/1riOGrE_Fv-yfIbM5V4pgJx4DWcd92cZr#scrollTo=ITaPDPIQEgXV) for running and naming your Colab notebooks.\n",
        "\n"
      ],
      "metadata": {
        "id": "IH6rN78lSy4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optioal) CONSENT.** If ok with sharing your Colab for educational purposes, please check the box below with \"X\".\n",
        "\n",
        "<mark>[X]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purpose. We understand that sharing is optional and this decision will not affect our grade in any way. "
      ],
      "metadata": {
        "id": "WNT5Kmk_TM1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if kaggle.json is stored in Google Drive"
      ],
      "metadata": {
        "id": "bj6xfvcnNdAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28146cde-77dd-458b-fcee-901f3ae05af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U sentence-transformers > log    # install sentence BERT text encoder\n",
        "!pip -q install -U --force-reinstall --no-deps kaggle >> log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                               # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >>log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json >> log      # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                  # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v oct312022moviegenres   # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log              # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                            # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show           # print public leaderboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgmrzIo9Ocs3",
        "outputId": "f191030d-a6e2-4cb5-9ddf-e317bca84388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: oct312022moviegenres\n",
            "Using competition: oct312022moviegenres\n",
            " teamId  teamName                     submissionDate       score    \n",
            "-------  ---------------------------  -------------------  -------  \n",
            "9407000  Gleb Kudriashov              2022-11-12 22:21:38  0.92697  \n",
            "9420438  üêØE-WhereIsTheMarkLebowski    2022-11-11 17:52:52  0.92181  \n",
            "9440142  üêØD-DwayneScalaJSON-Averin    2022-11-13 10:04:26  0.91952  \n",
            "9429852  Artem Borisov                2022-11-11 11:38:24  0.91552  \n",
            "9448666  üêØN-Voronina,Asharin,Almasya  2022-11-12 22:39:59  0.91552  \n",
            "9419878  üêØQ-RIPBILLY                  2022-11-12 09:28:09  0.91482  \n",
            "9417635  Artemiy Astashkin            2022-11-11 14:28:15  0.91461  \n",
            "9412491  Nikita Tyuplyaev             2022-11-12 22:30:55  0.91311  \n",
            "9379087  üêØI-ABC                       2022-11-12 18:26:36  0.91174  \n",
            "9421856  Nikita Aksenov               2022-11-12 22:34:30  0.91044  \n",
            "9417295  islambeg katibov             2022-11-09 20:14:46  0.90924  \n",
            "9448470  Travnikov Ivan               2022-11-12 11:11:41  0.90845  \n",
            "9447983  üêØJ                           2022-11-12 17:26:05  0.90678  \n",
            "9450139  üêØW-Plot                      2022-11-13 08:44:29  0.90666  \n",
            "9445069  üêØA-Team                      2022-11-12 17:12:27  0.90657  \n",
            "9420968  Turchin Ruslan               2022-11-11 11:37:45  0.90624  \n",
            "9449317  Anastasiia Prokhorova        2022-11-12 15:27:11  0.90457  \n",
            "9375412  üéûÔ∏èBaselineüêçRidgeClassifier   2022-10-30 09:53:01  0.90437  \n",
            "9429739  –ö–∞–ª–∏–Ω–∏–Ω–∞ –î–∞—Ä—å—è               2022-11-11 00:13:54  0.90437  \n",
            "9448585  Aleksandr Pak                2022-11-12 12:16:37  0.90399  \n",
            "9438707  Artem Kulikov                2022-11-10 17:36:52  0.90353  \n",
            "9444864  Kirill Barinov               2022-11-11 15:46:05  0.90278  \n",
            "9442986  Yuri Lyu                     2022-11-13 10:21:27  0.90183  \n",
            "9421491  fonarevaksenia               2022-11-10 00:04:18  0.90133  \n",
            "9420084  üêØAF-HAF                      2022-11-12 21:02:08  0.89446  \n",
            "9426127  Matvey Ryumin                2022-11-09 12:42:20  0.89238  \n",
            "9443902  harsh                        2022-11-12 22:15:23  0.82947  \n",
            "9450240  Mikhail Korzh                2022-11-13 08:46:51  0.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FgC70FIv_Lg"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS\n",
        "IS.ast_node_interactivity = \"all\"    # allows multiple outputs from a cell\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, plotly, time\n",
        "from sklearn.model_selection import train_test_split  # the only allowed function from sklearn\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV\n",
        "from sklearn.neighbors import RadiusNeighborsClassifier\n",
        "from sentence_transformers import SentenceTransformer as SBERT  # to encode multilingual text into numeric vectors\n",
        "\n",
        "pd.set_option('max_rows', 5, 'max_columns', 100, 'max_colwidth', 30, 'precision', 2)\n",
        "np.set_printoptions(linewidth=100, precision=2, edgeitems=5, suppress=True)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)\n",
        "\n",
        "XY = pd.read_csv('MovieGenresXY.csv')\n",
        "nClass = 20                                         # number of output classes/genres\n",
        "YCols = XY.columns[-nClass:]                        # 20 output columns - movie genre indicators\n",
        "# XNumCols = XY.select_dtypes(include=np.number).drop(YCols, axis=1).columns  # numeric column names\n",
        "# XY[XNumCols] = XY[XNumCols].fillna(0)               # fill numeric columns with zeros\n",
        "# XY['desc'] = XY.overview + '.' + XY.title + '. ' + XY.original_language + '. ' + XY.keywords + '. ' + XY.tagline\n",
        "# vX = XY.query('Action!=Action').drop(YCols, axis=1) # test inputs, movie attributes\n",
        "# tXY = XY.query('Action==Action')                    # training I/O\n",
        "# tX, tY = tXY.drop(YCols, axis=1), tXY[YCols]        # split into training I/O\n",
        "# vX, tX = vX.fillna('na'), tX.fillna('na')           # textual fields are filled with NA text\n",
        "# XY"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing some more libraries\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sns\n",
        "import ast "
      ],
      "metadata": {
        "id": "SPqAUQ4mr9TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmr = Timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6xBKV0KTTPT",
        "outputId": "d91a9244-553f-43a7-e765-3cba0992c961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ started. You have 60 sec. Good luck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "**Student's Section** (between ‚è≥ symbols): add your code and documentation here."
      ],
      "metadata": {
        "id": "-iirKmd7TR0t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        " \n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc. \n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements? \n",
        "1. What else have you tried that worked or didn't? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:**\n",
        "Of course, we considered all the textual information in the dataset and cleaned them. Instead of string with dictionary extracted tag words and so on. Also created dummies from companies and countries which produced the film + dummies on the decade of release, but the most benefit came from dummy on film companies creation. Other dummy variables didn't make significant improvement. Also searched for better performing BERT and it helped. Finally, worked with numeric variables, logged some of them and substituted some missing data (nans and zeros) onto something more reasonable. It gave only slight improvement. The performance of everything was checked by cross-val kaggle metric score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful. \n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements? \n",
        "1. What else have you tried that worked or didn't? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:** \n",
        "Tried all models, that we can use, but none was better than RidgeClassifier. I used CV to get alpha and selected features based on value of coef (since all vars are normalized, it was somehow reasonable), and feature selection gave a little bit of improvement, but maybe I should be more careful with overfit. So other models didn't work, effectiveness was again checked using cross-validation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJs0jS4fIO1j"
      },
      "source": [
        "Below is a baseline model that produces the result on Kaggle leaderboard (LB)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_ZjJeU5lTanD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XY = pd.read_csv('MovieGenresXY.csv')\n",
        "# nClass = 20                                         # number of output classes/genres\n",
        "# YCols = XY.columns[-nClass:]                        # 20 output columns - movie genre indicators\n",
        "\n",
        "# RUNTIME: nan and zeroes in runtime, filling in with avg value!!! BUT CHECK THE PERFORMANCE, MAYBE IT'S WORSE!!!\n",
        "XY['runtime'] = XY['runtime'].fillna(107)               # fill numeric columns with zeros\n",
        "XY['runtime'] = XY['runtime'].apply(lambda x: x + 107 if x == 0 else x)\n",
        "# the thing above gives slight improvement to the cross-val score\n",
        "\n",
        "# BUDGET: 1000 entries with less than budget, but definitely it should be larger, so replace to more logical vals\n",
        "  # I am gonna log this vector and fill with random variable from normal distribution to avoid too many single values\n",
        "# https://stackoverflow.com/questions/47497466/python-fill-na-in-pandas-column-with-random-elements-from-a-list\n",
        "# https://stackoverflow.com/questions/45416684/python-pandas-replace-multiple-columns-zero-to-nan\n",
        "mean = np.log(XY[XY['budget'] > 1000]['budget']).mean()\n",
        "std = np.log(XY[XY['budget'] > 1000]['budget']).std()\n",
        "XY['incor_budget'] = (XY['budget'] <= 1000).astype('float')\n",
        "# from less than 1000 to nan\n",
        "XY['budget_nan'] = XY['budget'].apply(lambda x: np.nan if x < 1000 else x)\n",
        "XY['log_budget'] = np.log(XY['budget_nan'])\n",
        "# filling nans with random var: normal mean and std\n",
        "np.random.seed(42)\n",
        "XY['log_budget'] = XY['log_budget'].fillna(pd.Series(np.random.normal(mean, std, size=len(XY.index))))\n",
        "XY = XY.drop('budget', axis = 1)\n",
        "XY = XY.drop('budget_nan', axis = 1)\n",
        "\n",
        "# doing the same for REVENUE\n",
        "mean = np.log(XY[XY['revenue'] > 1000]['revenue']).mean()\n",
        "std = np.log(XY[XY['revenue'] > 1000]['revenue']).std()\n",
        "XY['incor_revenue'] = (XY['revenue'] <= 1000).astype('float')\n",
        "# from less than 1000 to nan\n",
        "XY['revenue_nan'] = XY['revenue'].apply(lambda x: np.nan if x < 1000 else x)\n",
        "XY['log_revenue'] = np.log(XY['revenue_nan'])\n",
        "# filling nans with random var: normal mean and std\n",
        "np.random.seed(42)\n",
        "XY['log_revenue'] = XY['log_revenue'].fillna(pd.Series(np.random.normal(mean, std, size=len(XY.index))))\n",
        "# XY = XY.drop('revenue', axis = 1)\n",
        "XY = XY.drop('revenue_nan', axis = 1)\n",
        "\n",
        "# Popularity\n",
        "XY['log_popularity'] = np.log(1 + XY['popularity'])\n",
        "XY = XY.drop('popularity', axis = 1)\n",
        "\n",
        "# Vote_count\n",
        "XY['log_vote_count'] = np.log(1 + XY['vote_count'])\n",
        "XY = XY.drop('vote_count', axis = 1)\n",
        "\n",
        "# VERY SLIGHT IMPROVEMENT OVERALL AFTER NUMERIC COLS TRANSFORMATION\n",
        "\n",
        "\n",
        "# Text\n",
        "# for some obs, tagline is Nan, but it's not too big deal, but if it's nan, desc will be also nan!!!\n",
        "  # To avoid this I first remove nan and then add, so that desc for no tagliners will also exist!!!\n",
        "# the same for overview, but the scale is not so big (only 3 nans)\n",
        "XY['tagline'] = XY['tagline'].fillna('')\n",
        "XY['overview'] = XY['overview'].fillna('')\n",
        "\n",
        "# KEYWORDS (UPD: there was mistake, it gave improvement)\n",
        "# https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "def json_encoder(i):\n",
        "    li = ast.literal_eval(i)\n",
        "    try: \n",
        "      return ' '.join(list(pd.DataFrame(li)['name']))\n",
        "    except:\n",
        "      # when no keywords \n",
        "      return ''\n",
        "XY['keywords_new'] = XY['keywords'].map(json_encoder)\n",
        "\n",
        "# # ORIGINAL_LANGUAGE: many languages, but en dominant, You can make dummies for langs, non-english: if count in dataset > 10!!!\n",
        "#   # ALSO NUMBER OF LANGUAGES AS VARIABLE MAY WORK (DUMMIES)\n",
        "# # (slightly lowers cross-val score!!!)\n",
        "# dict_or_lang = dict(XY.original_language.value_counts())\n",
        "# # leaving only the languages with more than 10 observations\n",
        "# XY['original_language'] = XY['original_language'].apply(lambda x: x if dict_or_lang[x] > 10 else 'other')\n",
        "# # so that english is dropped in get dummies\n",
        "# XY['original_language'] = XY['original_language'].apply(lambda x: '0en' if x == 'en' else x)\n",
        "# XY = pd.get_dummies(XY, drop_first = True, columns = ['original_language'])\n",
        "\n",
        "# # Status: only 8 status non-released, no need to work with it!!! \n",
        "\n",
        "# PRODUCTION COMPANIES: 351 empty, others have company, actually can work with them, but not now I think!! But maybe work with names!!! LEAVE ONLY THOSE DUMMIY COMPANIES, WHICH HAVE >10 FILMS!!!!\n",
        "  # NUMER OF COMPANIES (DUMMIES) MAY ALSO HELP!!!\n",
        "# finds all companies for each observation\n",
        "def prod_companies_encode(i):\n",
        "    li = ast.literal_eval(i)\n",
        "    try: \n",
        "      return list(pd.DataFrame(li)['name'])\n",
        "    except:\n",
        "      # when no dictionary \n",
        "      return list()\n",
        "vec_companies = list(XY['production_companies'].map(prod_companies_encode))\n",
        "# all companies for all films glued together (leaving repetitions to find overall frequency of certain company!!!)\n",
        "all_comp = []\n",
        "comp_counts = []\n",
        "for i in vec_companies:\n",
        "  all_comp += i\n",
        "  comp_counts.append(len(i))\n",
        "# number of companies for each film!\n",
        "XY['comp_count'] = comp_counts\n",
        "# decided to make dummies: more than 0, 1, 2, 5\n",
        "knots = [0, 1, 2, 5]\n",
        "for num, i in enumerate(knots): \n",
        "    colname = 'comp_count_more_{}'.format(i)\n",
        "    if num != 3:\n",
        "        XY[colname] = ((XY['comp_count'] > i) & (XY['comp_count'] <= knots[num + 1])).astype('int')\n",
        "    else: \n",
        "        XY[colname] = (XY['comp_count'] > i).astype('int')\n",
        "# Frequency of each company overall!!!\n",
        "comp, count= np.unique(np.array(all_comp), return_counts = True)\n",
        "df_comps = pd.DataFrame(np.asarray((comp, count)).T)\n",
        "df_comps[1] = df_comps[1].astype('int')\n",
        "df_comps = df_comps.sort_values(1, ascending = False)\n",
        "# IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "df_comps_filt = df_comps[df_comps[1] > 10]\n",
        "# nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "comps_to_consider = list(df_comps_filt[0])\n",
        "# function to catch company in a string of list of dictionaries\n",
        "def company_dummy(i, company):\n",
        "    if company in i:\n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "# dummy columns creations\n",
        "for company in comps_to_consider: \n",
        "    colname = 'is_' + company\n",
        "    XY[colname] = XY['production_companies'].apply(lambda x: company_dummy(x, company))\n",
        "# Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "  # we can delete these columns!\n",
        "comp_cols_avg = XY.iloc[:, -len(comps_to_consider): ].mean(axis = 0)\n",
        "not_correct_comps = list(comp_cols_avg[comp_cols_avg == 0].index)\n",
        "XY = XY.drop(not_correct_comps, axis = 1)\n",
        "\n",
        "# # PRODUCTION COUNTRIES: SIMILAR AS FOR PRODUCTION COUNTRIES\n",
        "# def prod_countries_encode(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     try: \n",
        "#       return list(pd.DataFrame(li)['iso_3166_1'])\n",
        "#     except:\n",
        "#       # when no dictionary \n",
        "#       return list()\n",
        "# list_countries = list(XY['production_countries'].map(prod_countries_encode))\n",
        "# # all countries for all films glued together (leaving repetitions to find overall frequency of certain countries!!!)\n",
        "# all_countries = []\n",
        "# countries_counts = []\n",
        "# for i in list_countries:\n",
        "#   all_countries += i\n",
        "#   countries_counts.append(len(i))\n",
        "# # number of compa for each film!\n",
        "# XY['countries_count'] = countries_counts\n",
        "# # decided to make dummies: more than 0, 1, 2\n",
        "# knots = [0, 1, 2]\n",
        "# for num, i in enumerate(knots): \n",
        "#     colname = 'countries_count_more_{}'.format(i)\n",
        "#     if num != 2:\n",
        "#         XY[colname] = ((XY['countries_count'] > i) & (XY['countries_count'] <= knots[num + 1])).astype('int')\n",
        "#     else:\n",
        "#         XY[colname] = (XY['countries_count'] > i).astype('int')\n",
        "# # Frequency of each countries overall!!!\n",
        "# countries, count = np.unique(np.array(all_countries), return_counts = True)\n",
        "# df_countries = pd.DataFrame(np.asarray((countries, count)).T)\n",
        "# df_countries[1] = df_countries[1].astype('int')\n",
        "# df_countries = df_countries.sort_values(1, ascending = False)\n",
        "# # IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "# df_countries_filt = df_countries[df_countries[1] > 10]\n",
        "# # nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "# countries_to_consider = list(df_countries_filt[0])\n",
        "# # function to catch company in a string of list of dictionaries\n",
        "# def country_dummy(i, country):\n",
        "#     if country in i:\n",
        "#         return 1\n",
        "#     else: \n",
        "#         return 0\n",
        "# # dummy columns creations\n",
        "# for country in countries_to_consider: \n",
        "#     colname = 'is_' + country\n",
        "#     XY[colname] = XY['production_countries'].apply(lambda x: country_dummy(x, country))\n",
        "# # Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "#   # we can delete these columns!\n",
        "# countries_cols_avg = XY.iloc[:, -len(countries_to_consider): ].mean(axis = 0)\n",
        "# not_correct_countries = list(countries_cols_avg[countries_cols_avg == 0].index)\n",
        "# XY = XY.drop(not_correct_countries, axis = 1)\n",
        "\n",
        "# # SPOKEN LANGUAGE: SAME APPROACH AS PREVIOUS TWO VARIABLES!!!\n",
        "# def prod_lang_encode(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     try: \n",
        "#       return list(pd.DataFrame(li)['iso_639_1'])\n",
        "#     except:\n",
        "#       # when no dictionary \n",
        "#       return list()\n",
        "# list_lang = list(XY['spoken_languages'].map(prod_lang_encode))\n",
        "# # all countries for all films glued together (leaving repetitions to find overall frequency of certain countries!!!)\n",
        "# all_lang = []\n",
        "# lang_counts = []\n",
        "# for i in list_lang:\n",
        "#   all_lang += i\n",
        "#   lang_counts.append(len(i))\n",
        "# # number of compa for each film!\n",
        "# XY['lang_count'] = lang_counts\n",
        "# # decided to make dummies: more than 0, 1, 2\n",
        "# knots = [0, 1, 2]\n",
        "# for num, i in enumerate(knots): \n",
        "#     colname = 'lang_count_more_{}'.format(i)\n",
        "#     if num != 2:\n",
        "#         XY[colname] = ((XY['lang_count'] > i) & (XY['lang_count'] <= knots[num + 1])).astype('int')\n",
        "#     else: \n",
        "#         XY[colname] = (XY['lang_count'] > i).astype('int')\n",
        "# # Frequency of each countries overall!!!\n",
        "# lang, count = np.unique(np.array(all_lang), return_counts = True)\n",
        "# df_lang = pd.DataFrame(np.asarray((lang, count)).T)\n",
        "# df_lang[1] = df_lang[1].astype('int')\n",
        "# df_lang = df_lang.sort_values(1, ascending = False)\n",
        "# # IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "# df_lang_filt = df_lang[df_lang[1] > 10]\n",
        "# # nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "# lang_to_consider = list(df_lang_filt[0])\n",
        "# # function to catch company in a string of list of dictionaries\n",
        "# def lang_dummy(i, lang):\n",
        "#     if lang in i:\n",
        "#         return 1\n",
        "#     else: \n",
        "#         return 0\n",
        "# # dummy columns creations\n",
        "# for lang in lang_to_consider: \n",
        "#     colname = 'is_' + lang\n",
        "#     XY[colname] = XY['spoken_languages'].apply(lambda x: lang_dummy(x, lang))\n",
        "# # Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "#   # we can delete these columns!\n",
        "# lang_cols_avg = XY.iloc[:, -len(lang_to_consider): ].mean(axis = 0)\n",
        "# not_correct_lang = list(lang_cols_avg[lang_cols_avg == 0].index)\n",
        "# XY = XY.drop(not_correct_lang, axis = 1)\n",
        "\n",
        "# # RELEASE DATE - may create dummies based on decades!!\n",
        "# # https://stackoverflow.com/questions/17764619/pandas-dataframe-group-year-index-by-decade\n",
        "# # spoken_languages: based on codes, make dummies whether certain code is in dictionary!!!\n",
        "#   # NUMBER OF LANGUAGES(DUMMIES) MAY BE HELPFUL!\n",
        "# # DELETE ALL CREATED DUMMIES, IF NUMBER OF '1' IS LESS THAN 10 OR SOME OTHER BENCHMARK!!!\n",
        "# # only one film with NA, which was released in 2015!\n",
        "# XY['release_date'] = XY['release_date'].fillna('2015-01-01')\n",
        "# XY['release_date'] = pd.to_datetime(XY.release_date)\n",
        "# XY['decade_release'] = (XY.release_date.dt.year//10)*10\n",
        "# XY['decade_release_bef_1950'] = (XY['decade_release'] < 1950).astype('int')\n",
        "# XY['decade_release_1950_1960'] = ((XY['decade_release'] >= 1950) & (XY['decade_release'] < 1970)).astype('int')\n",
        "# XY['decade_release_70s'] = (XY['decade_release'] == 1970).astype('int')\n",
        "# XY['decade_release_80s'] = (XY['decade_release'] == 1980).astype('int')\n",
        "# XY['decade_release_90s'] = (XY['decade_release'] == 1990).astype('int')\n",
        "# XY['decade_release_2000s'] = (XY['decade_release'] == 2000).astype('int')\n",
        "\n",
        "# ok, the only numeric columns (with dummies from text variables)\n",
        "XNumCols = XY.select_dtypes(include=np.number).drop(YCols, axis=1).columns  # numeric column names\n",
        "\n",
        "XY['desc'] = XY.overview + '.' + XY.title + '. ' + XY.keywords_new + '. ' + XY.tagline\n",
        "vX = XY.query('Action!=Action').drop(YCols, axis=1) # test inputs, movie attributes\n",
        "tXY = XY.query('Action==Action')                    # training I/O\n",
        "tX, tY = tXY.drop(YCols, axis=1), tXY[YCols]        # split into training I/O\n",
        "vX, tX = vX.fillna('na'), tX.fillna('na')           # textual fields are filled with NA text\n",
        "XY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "pXFquXZYZbUi",
        "outputId": "0a36071b-ae7c-4f71-d7ff-442d85165f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:125: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           homepage                       keywords  \\\n",
              "0                               NaN  [{\"id\": 907, \"name\": \"japa...   \n",
              "1                               NaN  [{\"id\": 520, \"name\": \"chic...   \n",
              "...                             ...                            ...   \n",
              "4801  http://www.biglebowskiblur...  [{\"id\": 418, \"name\": \"whit...   \n",
              "4802                            NaN  [{\"id\": 4118, \"name\": \"bal...   \n",
              "\n",
              "     original_language       original_title                       overview  \\\n",
              "0                   en  Hachi: A Dog's Tale  A drama based on the true ...   \n",
              "1                   en            The Sting  Set in the 1930's this int...   \n",
              "...                ...                  ...                            ...   \n",
              "4801                en     The Big Lebowski  Jeffrey \"The Dude\" Lebowsk...   \n",
              "4802                en  Save the Last Dance  A white midwestern girl mo...   \n",
              "\n",
              "               production_companies           production_countries  \\\n",
              "0     [{\"name\": \"Grand Army Ente...  [{\"iso_3166_1\": \"GB\", \"nam...   \n",
              "1     [{\"name\": \"Universal Pictu...  [{\"iso_3166_1\": \"US\", \"nam...   \n",
              "...                             ...                            ...   \n",
              "4801  [{\"name\": \"Gramercy Pictur...  [{\"iso_3166_1\": \"GB\", \"nam...   \n",
              "4802  [{\"name\": \"MTV Films\", \"id...  [{\"iso_3166_1\": \"US\", \"nam...   \n",
              "\n",
              "     release_date    revenue  runtime               spoken_languages  \\\n",
              "0      2009-06-13   47801389     93.0  [{\"iso_639_1\": \"en\", \"name...   \n",
              "1      1973-12-25  159616327    129.0  [{\"iso_639_1\": \"en\", \"name...   \n",
              "...           ...        ...      ...                            ...   \n",
              "4801   1998-03-06   46189568    117.0  [{\"iso_639_1\": \"en\", \"name...   \n",
              "4802   2001-01-12   91038276    112.0  [{\"iso_639_1\": \"en\", \"name...   \n",
              "\n",
              "        status                        tagline                title  \\\n",
              "0     Released  A true story of faith, dev...  Hachi: A Dog's Tale   \n",
              "1     Released  ...all it takes is a littl...            The Sting   \n",
              "...        ...                            ...                  ...   \n",
              "4801  Released  Times like these call for ...     The Big Lebowski   \n",
              "4802  Released  The Only Person You Need T...  Save the Last Dance   \n",
              "\n",
              "      vote_average  Action  Adventure  Animation  Comedy  Crime  Documentary  \\\n",
              "0              7.7     NaN        NaN        NaN     NaN    NaN          NaN   \n",
              "1              7.9     NaN        NaN        NaN     NaN    NaN          NaN   \n",
              "...            ...     ...        ...        ...     ...    ...          ...   \n",
              "4801           7.8     0.0        0.0        0.0     1.0    1.0          0.0   \n",
              "4802           6.3     0.0        0.0        0.0     0.0    0.0          0.0   \n",
              "\n",
              "      Drama  Family  Fantasy  Foreign  History  Horror  Music  Mystery  \\\n",
              "0       NaN     NaN      NaN      NaN      NaN     NaN    NaN      NaN   \n",
              "1       NaN     NaN      NaN      NaN      NaN     NaN    NaN      NaN   \n",
              "...     ...     ...      ...      ...      ...     ...    ...      ...   \n",
              "4801    0.0     0.0      0.0      0.0      0.0     0.0    0.0      0.0   \n",
              "4802    1.0     1.0      0.0      0.0      0.0     0.0    1.0      0.0   \n",
              "\n",
              "      Romance  Science Fiction  TV Movie  Thriller  War  Western  \\\n",
              "0         NaN              NaN       NaN       NaN  NaN      NaN   \n",
              "1         NaN              NaN       NaN       NaN  NaN      NaN   \n",
              "...       ...              ...       ...       ...  ...      ...   \n",
              "4801      0.0              0.0       0.0       0.0  0.0      0.0   \n",
              "4802      1.0              0.0       0.0       0.0  0.0      0.0   \n",
              "\n",
              "      incor_budget  log_budget  incor_revenue  log_revenue  log_popularity  \\\n",
              "0              0.0       16.59            0.0        17.68            3.79   \n",
              "1              0.0       15.52            0.0        18.89            3.38   \n",
              "...            ...         ...            ...          ...             ...   \n",
              "4801           0.0       16.52            0.0        17.65            3.92   \n",
              "4802           0.0       16.38            0.0        18.33            2.40   \n",
              "\n",
              "      log_vote_count                   keywords_new  comp_count  \\\n",
              "0               7.45  japanese loyalty human ani...           6   \n",
              "1               6.43  chicago bet horse race rep...           1   \n",
              "...              ...                            ...         ...   \n",
              "4801            7.98  white russian dude bowling...           3   \n",
              "4802            5.87   ballet dancer musical ballet           2   \n",
              "\n",
              "      comp_count_more_0  comp_count_more_1  comp_count_more_2  \\\n",
              "0                     0                  0                  0   \n",
              "1                     1                  0                  0   \n",
              "...                 ...                ...                ...   \n",
              "4801                  0                  0                  1   \n",
              "4802                  0                  1                  0   \n",
              "\n",
              "      comp_count_more_5  is_Warner Bros.  is_Universal Pictures  \\\n",
              "0                     1                0                      0   \n",
              "1                     0                0                      1   \n",
              "...                 ...              ...                    ...   \n",
              "4801                  0                0                      0   \n",
              "4802                  0                0                      0   \n",
              "\n",
              "      is_Paramount Pictures  ...  is_Atlas Entertainment  \\\n",
              "0                         0  ...                       0   \n",
              "1                         0  ...                       0   \n",
              "...                     ...  ...                     ...   \n",
              "4801                      0  ...                       0   \n",
              "4802                      0  ...                       0   \n",
              "\n",
              "      is_Mel's Cite du Cinema  is_Paramount Vantage  \\\n",
              "0                           0                     0   \n",
              "1                           0                     0   \n",
              "...                       ...                   ...   \n",
              "4801                        0                     0   \n",
              "4802                        0                     0   \n",
              "\n",
              "      is_Robert Simonds Productions  is_Marvel Studios  is_Channel Four Films  \\\n",
              "0                                 0                  0                      0   \n",
              "1                                 0                  0                      0   \n",
              "...                             ...                ...                    ...   \n",
              "4801                              0                  0                      0   \n",
              "4802                              0                  0                      0   \n",
              "\n",
              "      is_2929 Productions  is_Nu Image Films  is_Saturn Films  is_Scion Films  \\\n",
              "0                       0                  0                0               1   \n",
              "1                       0                  0                0               0   \n",
              "...                   ...                ...              ...             ...   \n",
              "4801                    0                  0                0               0   \n",
              "4802                    0                  0                0               0   \n",
              "\n",
              "      is_RatPac-Dune Entertainment  is_Kennedy/Marshall Company, The  \\\n",
              "0                                0                              0      \n",
              "1                                0                              0      \n",
              "...                            ...                            ...      \n",
              "4801                             0                              0      \n",
              "4802                             0                              0      \n",
              "\n",
              "      is_BenderSpink  is_Tim Burton Productions  is_Sunswept Entertainment  \\\n",
              "0                  0                          0                          0   \n",
              "1                  0                          0                          0   \n",
              "...              ...                        ...                        ...   \n",
              "4801               0                          0                          0   \n",
              "4802               0                          0                          0   \n",
              "\n",
              "      is_3 Arts Entertainment  is_Goldcrest Pictures  \\\n",
              "0                           0                      0   \n",
              "1                           0                      0   \n",
              "...                       ...                    ...   \n",
              "4801                        0                      0   \n",
              "4802                        0                      0   \n",
              "\n",
              "      is_Bad Hat Harry Productions  is_LStar Capital  \\\n",
              "0                                0                 0   \n",
              "1                                0                 0   \n",
              "...                            ...               ...   \n",
              "4801                             0                 0   \n",
              "4802                             0                 0   \n",
              "\n",
              "      is_Moving Picture Company (MPC)  is_Gold Circle Films  is_CBS Films  \\\n",
              "0                                 0                       0             0   \n",
              "1                                 0                       0             0   \n",
              "...                             ...                     ...           ...   \n",
              "4801                              0                       0             0   \n",
              "4802                              0                       0             0   \n",
              "\n",
              "      is_Media Rights Capital  is_Worldview Entertainment  \\\n",
              "0                           0                           0   \n",
              "1                           0                           0   \n",
              "...                       ...                         ...   \n",
              "4801                        0                           0   \n",
              "4802                        0                           0   \n",
              "\n",
              "      is_Icon Entertainment International  is_The Kennedy/Marshall Company  \\\n",
              "0                                 0                                    0     \n",
              "1                                 0                                    0     \n",
              "...                             ...                                  ...     \n",
              "4801                              0                                    0     \n",
              "4802                              0                                    0     \n",
              "\n",
              "      is_Dentsu  is_Impact Pictures  is_Cinergi Pictures Entertainment  \\\n",
              "0             0                   0                              0       \n",
              "1             0                   0                              0       \n",
              "...         ...                 ...                            ...       \n",
              "4801          0                   0                              0       \n",
              "4802          0                   0                              0       \n",
              "\n",
              "      is_Mandate Pictures  is_Emmett/Furla Films  is_Endgame Entertainment  \\\n",
              "0                       0                      0                         0   \n",
              "1                       0                      0                         0   \n",
              "...                   ...                    ...                       ...   \n",
              "4801                    0                      0                         0   \n",
              "4802                    0                      0                         0   \n",
              "\n",
              "      is_21 Laps Entertainment  is_Davis-Films  is_DC Entertainment  \\\n",
              "0                            0               0                    0   \n",
              "1                            0               0                    0   \n",
              "...                        ...             ...                  ...   \n",
              "4801                         0               0                    0   \n",
              "4802                         0               0                    0   \n",
              "\n",
              "      is_Sony Pictures Animation  is_Destination Films  \\\n",
              "0                              0                     0   \n",
              "1                              0                     0   \n",
              "...                          ...                   ...   \n",
              "4801                           0                     0   \n",
              "4802                           0                     0   \n",
              "\n",
              "      is_British Broadcasting Corporation (BBC)  is_Good Machine  \\\n",
              "0                                 0                            0   \n",
              "1                                 0                            0   \n",
              "...                             ...                          ...   \n",
              "4801                              0                            0   \n",
              "4802                              0                            0   \n",
              "\n",
              "      is_Marc Platt Productions  is_Paramount Animation  \\\n",
              "0                             0                       0   \n",
              "1                             0                       0   \n",
              "...                         ...                     ...   \n",
              "4801                          0                       0   \n",
              "4802                          0                       0   \n",
              "\n",
              "      is_Twentieth Century Fox Animation  is_Fine Line Features  is_Miramax  \\\n",
              "0                                 0                           0           0   \n",
              "1                                 0                           0           0   \n",
              "...                             ...                         ...         ...   \n",
              "4801                              0                           0           0   \n",
              "4802                              0                           0           0   \n",
              "\n",
              "      is_Mandeville Films  is_WingNut Films  is_IM Global  is_DiNovi Pictures  \\\n",
              "0                       0                 0             0                   0   \n",
              "1                       0                 0             0                   0   \n",
              "...                   ...               ...           ...                 ...   \n",
              "4801                    0                 0             0                   0   \n",
              "4802                    0                 0             0                   0   \n",
              "\n",
              "      is_Alliance Atlantis Communications                           desc  \n",
              "0                                 0        A drama based on the true ...  \n",
              "1                                 0        Set in the 1930's this int...  \n",
              "...                             ...                                  ...  \n",
              "4801                              0        Jeffrey \"The Dude\" Lebowsk...  \n",
              "4802                              0        A white midwestern girl mo...  \n",
              "\n",
              "[4803 rows x 206 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-edd42bbc-30d0-4bec-a399-91aa28c8e1a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>homepage</th>\n",
              "      <th>keywords</th>\n",
              "      <th>original_language</th>\n",
              "      <th>original_title</th>\n",
              "      <th>overview</th>\n",
              "      <th>production_companies</th>\n",
              "      <th>production_countries</th>\n",
              "      <th>release_date</th>\n",
              "      <th>revenue</th>\n",
              "      <th>runtime</th>\n",
              "      <th>spoken_languages</th>\n",
              "      <th>status</th>\n",
              "      <th>tagline</th>\n",
              "      <th>title</th>\n",
              "      <th>vote_average</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Family</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Foreign</th>\n",
              "      <th>History</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Music</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Science Fiction</th>\n",
              "      <th>TV Movie</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "      <th>incor_budget</th>\n",
              "      <th>log_budget</th>\n",
              "      <th>incor_revenue</th>\n",
              "      <th>log_revenue</th>\n",
              "      <th>log_popularity</th>\n",
              "      <th>log_vote_count</th>\n",
              "      <th>keywords_new</th>\n",
              "      <th>comp_count</th>\n",
              "      <th>comp_count_more_0</th>\n",
              "      <th>comp_count_more_1</th>\n",
              "      <th>comp_count_more_2</th>\n",
              "      <th>comp_count_more_5</th>\n",
              "      <th>is_Warner Bros.</th>\n",
              "      <th>is_Universal Pictures</th>\n",
              "      <th>is_Paramount Pictures</th>\n",
              "      <th>...</th>\n",
              "      <th>is_Atlas Entertainment</th>\n",
              "      <th>is_Mel's Cite du Cinema</th>\n",
              "      <th>is_Paramount Vantage</th>\n",
              "      <th>is_Robert Simonds Productions</th>\n",
              "      <th>is_Marvel Studios</th>\n",
              "      <th>is_Channel Four Films</th>\n",
              "      <th>is_2929 Productions</th>\n",
              "      <th>is_Nu Image Films</th>\n",
              "      <th>is_Saturn Films</th>\n",
              "      <th>is_Scion Films</th>\n",
              "      <th>is_RatPac-Dune Entertainment</th>\n",
              "      <th>is_Kennedy/Marshall Company, The</th>\n",
              "      <th>is_BenderSpink</th>\n",
              "      <th>is_Tim Burton Productions</th>\n",
              "      <th>is_Sunswept Entertainment</th>\n",
              "      <th>is_3 Arts Entertainment</th>\n",
              "      <th>is_Goldcrest Pictures</th>\n",
              "      <th>is_Bad Hat Harry Productions</th>\n",
              "      <th>is_LStar Capital</th>\n",
              "      <th>is_Moving Picture Company (MPC)</th>\n",
              "      <th>is_Gold Circle Films</th>\n",
              "      <th>is_CBS Films</th>\n",
              "      <th>is_Media Rights Capital</th>\n",
              "      <th>is_Worldview Entertainment</th>\n",
              "      <th>is_Icon Entertainment International</th>\n",
              "      <th>is_The Kennedy/Marshall Company</th>\n",
              "      <th>is_Dentsu</th>\n",
              "      <th>is_Impact Pictures</th>\n",
              "      <th>is_Cinergi Pictures Entertainment</th>\n",
              "      <th>is_Mandate Pictures</th>\n",
              "      <th>is_Emmett/Furla Films</th>\n",
              "      <th>is_Endgame Entertainment</th>\n",
              "      <th>is_21 Laps Entertainment</th>\n",
              "      <th>is_Davis-Films</th>\n",
              "      <th>is_DC Entertainment</th>\n",
              "      <th>is_Sony Pictures Animation</th>\n",
              "      <th>is_Destination Films</th>\n",
              "      <th>is_British Broadcasting Corporation (BBC)</th>\n",
              "      <th>is_Good Machine</th>\n",
              "      <th>is_Marc Platt Productions</th>\n",
              "      <th>is_Paramount Animation</th>\n",
              "      <th>is_Twentieth Century Fox Animation</th>\n",
              "      <th>is_Fine Line Features</th>\n",
              "      <th>is_Miramax</th>\n",
              "      <th>is_Mandeville Films</th>\n",
              "      <th>is_WingNut Films</th>\n",
              "      <th>is_IM Global</th>\n",
              "      <th>is_DiNovi Pictures</th>\n",
              "      <th>is_Alliance Atlantis Communications</th>\n",
              "      <th>desc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[{\"id\": 907, \"name\": \"japa...</td>\n",
              "      <td>en</td>\n",
              "      <td>Hachi: A Dog's Tale</td>\n",
              "      <td>A drama based on the true ...</td>\n",
              "      <td>[{\"name\": \"Grand Army Ente...</td>\n",
              "      <td>[{\"iso_3166_1\": \"GB\", \"nam...</td>\n",
              "      <td>2009-06-13</td>\n",
              "      <td>47801389</td>\n",
              "      <td>93.0</td>\n",
              "      <td>[{\"iso_639_1\": \"en\", \"name...</td>\n",
              "      <td>Released</td>\n",
              "      <td>A true story of faith, dev...</td>\n",
              "      <td>Hachi: A Dog's Tale</td>\n",
              "      <td>7.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.68</td>\n",
              "      <td>3.79</td>\n",
              "      <td>7.45</td>\n",
              "      <td>japanese loyalty human ani...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A drama based on the true ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[{\"id\": 520, \"name\": \"chic...</td>\n",
              "      <td>en</td>\n",
              "      <td>The Sting</td>\n",
              "      <td>Set in the 1930's this int...</td>\n",
              "      <td>[{\"name\": \"Universal Pictu...</td>\n",
              "      <td>[{\"iso_3166_1\": \"US\", \"nam...</td>\n",
              "      <td>1973-12-25</td>\n",
              "      <td>159616327</td>\n",
              "      <td>129.0</td>\n",
              "      <td>[{\"iso_639_1\": \"en\", \"name...</td>\n",
              "      <td>Released</td>\n",
              "      <td>...all it takes is a littl...</td>\n",
              "      <td>The Sting</td>\n",
              "      <td>7.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.89</td>\n",
              "      <td>3.38</td>\n",
              "      <td>6.43</td>\n",
              "      <td>chicago bet horse race rep...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Set in the 1930's this int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4801</th>\n",
              "      <td>http://www.biglebowskiblur...</td>\n",
              "      <td>[{\"id\": 418, \"name\": \"whit...</td>\n",
              "      <td>en</td>\n",
              "      <td>The Big Lebowski</td>\n",
              "      <td>Jeffrey \"The Dude\" Lebowsk...</td>\n",
              "      <td>[{\"name\": \"Gramercy Pictur...</td>\n",
              "      <td>[{\"iso_3166_1\": \"GB\", \"nam...</td>\n",
              "      <td>1998-03-06</td>\n",
              "      <td>46189568</td>\n",
              "      <td>117.0</td>\n",
              "      <td>[{\"iso_639_1\": \"en\", \"name...</td>\n",
              "      <td>Released</td>\n",
              "      <td>Times like these call for ...</td>\n",
              "      <td>The Big Lebowski</td>\n",
              "      <td>7.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.65</td>\n",
              "      <td>3.92</td>\n",
              "      <td>7.98</td>\n",
              "      <td>white russian dude bowling...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Jeffrey \"The Dude\" Lebowsk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4802</th>\n",
              "      <td>NaN</td>\n",
              "      <td>[{\"id\": 4118, \"name\": \"bal...</td>\n",
              "      <td>en</td>\n",
              "      <td>Save the Last Dance</td>\n",
              "      <td>A white midwestern girl mo...</td>\n",
              "      <td>[{\"name\": \"MTV Films\", \"id...</td>\n",
              "      <td>[{\"iso_3166_1\": \"US\", \"nam...</td>\n",
              "      <td>2001-01-12</td>\n",
              "      <td>91038276</td>\n",
              "      <td>112.0</td>\n",
              "      <td>[{\"iso_639_1\": \"en\", \"name...</td>\n",
              "      <td>Released</td>\n",
              "      <td>The Only Person You Need T...</td>\n",
              "      <td>Save the Last Dance</td>\n",
              "      <td>6.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.33</td>\n",
              "      <td>2.40</td>\n",
              "      <td>5.87</td>\n",
              "      <td>ballet dancer musical ballet</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A white midwestern girl mo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4803 rows √ó 206 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-edd42bbc-30d0-4bec-a399-91aa28c8e1a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-edd42bbc-30d0-4bec-a399-91aa28c8e1a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-edd42bbc-30d0-4bec-a399-91aa28c8e1a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking numeric columns for outliers (all of the final changes will be done above!!!)"
      ],
      "metadata": {
        "id": "8_XuOxkkym4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BUDGET: 1000 entries with less than budget, but definitely it should be larger, so replace to more logical vals\n",
        "  # I am gonna log this vector and fill with random variable from normal distribution to avoid too many single values\n",
        "# https://stackoverflow.com/questions/47497466/python-fill-na-in-pandas-column-with-random-elements-from-a-list\n",
        "# https://stackoverflow.com/questions/45416684/python-pandas-replace-multiple-columns-zero-to-nan\n",
        "\n",
        "# mean = np.log(XY[XY['budget'] > 1000]['budget']).mean()\n",
        "# std = np.log(XY[XY['budget'] > 1000]['budget']).std()\n",
        "# # from less than 1000 to nan\n",
        "# XY['budget'] = XY['budget'].apply(lambda x: np.nan if x < 1000 else x)\n",
        "# XY['log_budget'] = np.log(XY['budget'])\n",
        "# # filling nans with random var: normal mean and std\n",
        "# np.random.seed(42)\n",
        "# XY['log_budget'] = XY['log_budget'].fillna(pd.Series(np.random.normal(mean, std, size=len(XY.index))))\n",
        "# XY = XY.drop('budget', axis = 1)\n",
        "\n",
        "# sns.kdeplot(XY['log_budget'])\n",
        "\n",
        "# POPULARITY: No outliers in my opinion, can just log\n",
        "# sns.histplot(np.log(1 + XY['popularity']))\n",
        "\n",
        "# also maybe makes sense to log VOTE_COUNT!\n",
        "\n",
        "# doing the same for revenue as for budget!!!!\n",
        "# mean = np.log(XY[XY['revenue'] > 1000]['revenue']).mean()\n",
        "# std = np.log(XY[XY['revenue'] > 1000]['revenue']).std()\n",
        "# # from less than 1000 to nan\n",
        "# XY['revenue'] = XY['revenue'].apply(lambda x: np.nan if x < 1000 else x)\n",
        "# XY['log_budget'] = np.log(XY['revenue'])\n",
        "# # filling nans with random var: normal mean and std\n",
        "# np.random.seed(42)\n",
        "# XY['log_revenue'] = XY['log_revenue'].fillna(pd.Series(np.random.normal(mean, std, size=len(XY.index))))\n",
        "# XY = XY.drop('revenue', axis = 1)\n",
        "# sns.histplot(XY['revenue'])"
      ],
      "metadata": {
        "id": "94t-TfO6zK2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with textual columns"
      ],
      "metadata": {
        "id": "Ed1eXM_iDVZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ORIGINAL LANGUAGE would be translated to dummies\n",
        "# For OVERVIEW AND TAGLINE AND TITLE it should be fine (removed nans onto '' so that single Nans will not destroy desc col)\n",
        "# As for KEYWORDS, I need to extract them, they are in messy format!!!\n",
        "# XY['desc'] = XY.overview + '.' + XY.title +  '. ' + XY.keywords + '. ' + XY.tagline\n",
        "# XY.overview[0]"
      ],
      "metadata": {
        "id": "h9MzadT9DZbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://stackoverflow.com/questions/1894269/how-to-convert-string-representation-of-list-to-a-list\n",
        "# def json_encoder(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     return ' '.join(list(pd.DataFrame(li)['name']))\n",
        "\n",
        "# XY['keywords_new'] = XY['keywords'].map(json_encoder)"
      ],
      "metadata": {
        "id": "oyGusU8NGjmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRHw6jZ6fjrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ORIGINAL_LANGUAGE: many languages, but en dominant, You can make dummies for langs, non-english: if count in dataset > 10!!!\n",
        "#   # ALSO NUMBER OF LANGUAGES AS VARIABLE MAY WORK (DUMMIES)\n",
        "# dict_or_lang = dict(XY.original_language.value_counts())\n",
        "# # leaving only the languages with more than 10 observations\n",
        "# XY['original_language'] = XY['original_language'].apply(lambda x: x if dict_or_lang[x] > 10 else 'other')\n",
        "# # so that english is dropped in get dummies\n",
        "# XY['original_language'] = XY['original_language'].apply(lambda x: '0en' if x == 'en' else x)\n",
        "# pd.DataFrame(XY['original_language'].value_counts())\n",
        "# XY = pd.get_dummies(XY, drop_first = True, columns = ['original_language'])\n"
      ],
      "metadata": {
        "id": "L_tgHmtYJvjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Status: only 8 status non-released, no need to work with it!!! \n",
        "\n",
        "# PRODUCTION COMPANIES: 351 empty, others have company, actually can work with them, but not now I think!! But maybe work with names!!! LEAVE ONLY THOSE DUMMIY COMPANIES, WHICH HAVE >10 FILMS!!!!\n",
        "  # NUMER OF COMPANIES (DUMMIES) MAY ALSO HELP!!!\n",
        "# finds all companies for each observation\n",
        "# def prod_companies_encode(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     try: \n",
        "#       return list(pd.DataFrame(li)['name'])\n",
        "#     except:\n",
        "#       # when no dictionary \n",
        "#       return list()\n",
        "# vec_companies = list(XY['production_companies'].map(prod_companies_encode))\n",
        "# # all companies for all films glued together (leaving repetitions to find overall frequency of certain company!!!)\n",
        "# all_comp = []\n",
        "# comp_counts = []\n",
        "# for i in vec_companies:\n",
        "#   all_comp += i\n",
        "#   comp_counts.append(len(i))\n",
        "# # number of companies for each film!\n",
        "# XY['comp_count'] = comp_counts\n",
        "# # decided to make dummies: more than 0, 1, 2, 5\n",
        "# for i in [0, 1, 2, 5]: \n",
        "#     colname = 'comp_count_more_{}'.format(i)\n",
        "#     XY[colname] = XY['comp_count'] > i\n",
        "\n",
        "# # Number of each company overall!!!\n",
        "# comp, count= np.unique(np.array(all_comp), return_counts = True)\n",
        "# df_comps = pd.DataFrame(np.asarray((comp, count)).T)\n",
        "# df_comps[1] = df_comps[1].astype('int')\n",
        "# df_comps = df_comps.sort_values(1, ascending = False)\n",
        "# # IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "# df_comps_filt = df_comps[df_comps[1] > 10]\n",
        "# # nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "# comps_to_consider = list(df_comps_filt[0])\n",
        "# # function to catch company in a string of list of dictionaries\n",
        "# def company_dummy(i, company):\n",
        "#     if company in i:\n",
        "#         return 1\n",
        "#     else: \n",
        "#         return 0\n",
        "\n",
        "# # dummy columns creations\n",
        "# for company in comps_to_consider: \n",
        "#     colname = 'is_' + company\n",
        "#     XY[colname] = XY['production_companies'].apply(lambda x: company_dummy(x, company))\n",
        "# # Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "#   # we can delete these columns!\n",
        "# comp_cols_avg = XY.iloc[:, -len(comps_to_consider): ].mean(axis = 0)\n",
        "# not_correct_comps = list(comp_cols_avg[comp_cols_avg == 0].index)\n",
        "# XY = XY.drop(not_correct_comps, axis = 1)"
      ],
      "metadata": {
        "id": "_RTlHiE76-YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # PRODUCTION COUNTRIES!!!\n",
        "# def prod_countries_encode(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     try: \n",
        "#       return list(pd.DataFrame(li)['iso_3166_1'])\n",
        "#     except:\n",
        "#       # when no dictionary \n",
        "#       return list()\n",
        "# list_countries = list(XY['production_countries'].map(prod_countries_encode))\n",
        "# # all countries for all films glued together (leaving repetitions to find overall frequency of certain countries!!!)\n",
        "# all_countries = []\n",
        "# countries_counts = []\n",
        "# for i in list_countries:\n",
        "#   all_countries += i\n",
        "#   countries_counts.append(len(i))\n",
        "# # number of compa for each film!\n",
        "# XY['countries_count'] = countries_counts\n",
        "# # decided to make dummies: more than 0, 1, 2\n",
        "# for i in [0, 1, 2]: \n",
        "#     colname = 'countries_count_more_{}'.format(i)\n",
        "#     XY[colname] = (XY['countries_count'] > i).astype('int')\n",
        "# # Frequency of each countries overall!!!\n",
        "# countries, count = np.unique(np.array(all_countries), return_counts = True)\n",
        "# df_countries = pd.DataFrame(np.asarray((countries, count)).T)\n",
        "# df_countries[1] = df_countries[1].astype('int')\n",
        "# df_countries = df_countries.sort_values(1, ascending = False)\n",
        "# # IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "# df_countries_filt = df_countries[df_countries[1] > 10]\n",
        "# # nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "# countries_to_consider = list(df_countries_filt[0])\n",
        "# # function to catch company in a string of list of dictionaries\n",
        "# def country_dummy(i, country):\n",
        "#     if country in i:\n",
        "#         return 1\n",
        "#     else: \n",
        "#         return 0\n",
        "# # dummy columns creations\n",
        "# for country in countries_to_consider: \n",
        "#     colname = 'is_' + country\n",
        "#     XY[colname] = XY['production_countries'].apply(lambda x: country_dummy(x, country))\n",
        "# # Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "#   # we can delete these columns!\n",
        "# countries_cols_avg = XY.iloc[:, -len(countries_to_consider): ].mean(axis = 0)\n",
        "# not_correct_countries = list(countries_cols_avg[countries_cols_avg == 0].index)\n",
        "# XY = XY.drop(not_correct_countries, axis = 1)"
      ],
      "metadata": {
        "id": "FCreu6eJgN-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # SPOKEN LANGUAGE: SAME APPROACH AS PREVIOUS TWO VARIABLES!!!\n",
        "# def prod_lang_encode(i):\n",
        "#     li = ast.literal_eval(i)\n",
        "#     try: \n",
        "#       return list(pd.DataFrame(li)['iso_639_1'])\n",
        "#     except:\n",
        "#       # when no dictionary \n",
        "#       return list()\n",
        "# list_lang = list(XY['spoken_languages'].map(prod_lang_encode))\n",
        "# # all countries for all films glued together (leaving repetitions to find overall frequency of certain countries!!!)\n",
        "# all_lang = []\n",
        "# lang_counts = []\n",
        "# for i in list_lang:\n",
        "#   all_lang += i\n",
        "#   lang_counts.append(len(i))\n",
        "# # number of compa for each film!\n",
        "# XY['lang_count'] = lang_counts\n",
        "# # decided to make dummies: more than 0, 1, 2\n",
        "# for i in [0, 1, 2]: \n",
        "#     colname = 'lang_count_more_{}'.format(i)\n",
        "#     XY[colname] = (XY['lang_count'] > i).astype('int')\n",
        "# # Frequency of each countries overall!!!\n",
        "# lang, count = np.unique(np.array(all_lang), return_counts = True)\n",
        "# df_lang = pd.DataFrame(np.asarray((lang, count)).T)\n",
        "# df_lang[1] = df_lang[1].astype('int')\n",
        "# df_lang = df_lang.sort_values(1, ascending = False)\n",
        "# # IF RUNS TOO LONG, CONSIDER CHANGING THE MIN NUMBER OF FILMS PRODUCED!!!!\n",
        "# df_lang_filt = df_lang[df_lang[1] > 10]\n",
        "# # nice 163 companies: can take all of them and make dummy on companies!!!\n",
        "# lang_to_consider = list(df_lang_filt[0])\n",
        "# # function to catch company in a string of list of dictionaries\n",
        "# def lang_dummy(i, lang):\n",
        "#     if lang in i:\n",
        "#         return 1\n",
        "#     else: \n",
        "#         return 0\n",
        "# # dummy columns creations\n",
        "# for lang in lang_to_consider: \n",
        "#     colname = 'is_' + lang\n",
        "#     XY[colname] = XY['spoken_languages'].apply(lambda x: lang_dummy(x, lang))\n",
        "# # Well, 5 companies are not done properly, due to French letters, but it's 5/163 is not big deal in this case\n",
        "#   # we can delete these columns!\n",
        "# lang_cols_avg = XY.iloc[:, -len(lang_to_consider): ].mean(axis = 0)\n",
        "# not_correct_lang = list(lang_cols_avg[lang_cols_avg == 0].index)\n",
        "# XY = XY.drop(not_correct_lang, axis = 1)"
      ],
      "metadata": {
        "id": "qtIjaxFVpGz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELEASE DATE - may create dummies based on decades!!\n",
        "# https://stackoverflow.com/questions/17764619/pandas-dataframe-group-year-index-by-decade\n",
        "# spoken_languages: based on codes, make dummies whether certain code is in dictionary!!!\n",
        "  # NUMBER OF LANGUAGES(DUMMIES) MAY BE HELPFUL!\n",
        "\n",
        "# DELETE ALL CREATED DUMMIES, IF NUMBER OF '1' IS LESS THAN 10 OR SOME OTHER BENCHMARK!!!\n",
        "\n",
        "# only one film with NA, which was released in 2015!\n",
        "# XY['release_date'] = XY['release_date'].fillna('2015-01-01')\n",
        "# XY['release_date'] = pd.to_datetime(XY.release_date)\n",
        "# XY['decade_release'] = (XY.release_date.dt.year//10)*10\n",
        "# XY['decade_release_bef_1950'] = XY['decade_release'] < 1950\n",
        "# XY['decade_release_1950_1960'] = (XY['decade_release'] >= 1950) & (XY['decade_release'] < 1970)\n",
        "# XY['decade_release_70s'] = XY['decade_release'] == 1970\n",
        "# XY['decade_release_80s'] = XY['decade_release'] == 1980\n",
        "# XY['decade_release_90s'] = XY['decade_release'] == 1990\n",
        "# XY['decade_release_2000s'] = XY['decade_release'] == 2000"
      ],
      "metadata": {
        "id": "O-nfQzGgf8L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding part!!"
      ],
      "metadata": {
        "id": "dW9BxEAlDZun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[SBERT](https://www.sbert.net) generates 384-dimensional text embedding vectors for each movie's description. See [more models](https://www.sbert.net/docs/pretrained_models.html). Use GPU runtime in Colab for 10-100x speed up."
      ],
      "metadata": {
        "id": "xMLVyaCvBHoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial: paraphrase-MiniLM-L6-v2\n",
        "\n",
        "# new embedding\n",
        "%time sbert = SBERT('all-mpnet-base-v2')  # load SBERT embedings model to encode textual fields"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZIclF-4w7Q3",
        "outputId": "51c70eb3-9c64-476d-db6b-049031891350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.04 s, sys: 514 ms, total: 1.56 s\n",
            "Wall time: 1.73 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "train_encode = sbert.encode(tX.desc.tolist())\n",
        "val_encode = sbert.encode(vX.desc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_QxF3kc9INk",
        "outputId": "f88e8ad4-4904-4f6c-a23a-747c5b83a595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27.2 s, sys: 204 ms, total: 27.4 s\n",
            "Wall time: 32.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual embedding\n",
        "tXEmb = np.c_[tX[XNumCols].values, train_encode] # Set GPU runtime in Colab for 10-100x speed up\n",
        "vXEmb = np.c_[vX[XNumCols].values, val_encode]\n",
        "print(f'Train embedding matrix size:', tXEmb.shape)\n",
        "pd.DataFrame(tXEmb[:3,:20], index=tXY.title[:3]).style.background_gradient(cmap='coolwarm')  # show movie description and a few of its embedding features"
      ],
      "metadata": {
        "id": "KrZmx5HtzBe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "f664e774-554e-4b39-e8c1-4a09688ae162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train embedding matrix size: (2401, 940)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f2aec383250>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_53c67_row0_col0 {\n",
              "  background-color: #aec9fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_53c67_row0_col1, #T_53c67_row0_col4, #T_53c67_row0_col7, #T_53c67_row0_col8, #T_53c67_row0_col9, #T_53c67_row0_col12, #T_53c67_row1_col2, #T_53c67_row1_col3, #T_53c67_row1_col5, #T_53c67_row1_col6, #T_53c67_row1_col12, #T_53c67_row2_col0, #T_53c67_row2_col10 {\n",
              "  background-color: #b40426;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_53c67_row0_col2 {\n",
              "  background-color: #efcebd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_53c67_row0_col3, #T_53c67_row0_col5, #T_53c67_row0_col6, #T_53c67_row0_col10, #T_53c67_row0_col11, #T_53c67_row0_col13, #T_53c67_row0_col14, #T_53c67_row0_col15, #T_53c67_row0_col16, #T_53c67_row0_col17, #T_53c67_row0_col18, #T_53c67_row0_col19, #T_53c67_row1_col0, #T_53c67_row1_col1, #T_53c67_row1_col10, #T_53c67_row1_col11, #T_53c67_row1_col13, #T_53c67_row1_col14, #T_53c67_row1_col15, #T_53c67_row1_col16, #T_53c67_row1_col17, #T_53c67_row1_col18, #T_53c67_row1_col19, #T_53c67_row2_col2, #T_53c67_row2_col3, #T_53c67_row2_col4, #T_53c67_row2_col5, #T_53c67_row2_col7, #T_53c67_row2_col8, #T_53c67_row2_col9, #T_53c67_row2_col11, #T_53c67_row2_col12, #T_53c67_row2_col13, #T_53c67_row2_col14, #T_53c67_row2_col15, #T_53c67_row2_col16, #T_53c67_row2_col17, #T_53c67_row2_col18, #T_53c67_row2_col19 {\n",
              "  background-color: #3b4cc0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_53c67_row1_col4 {\n",
              "  background-color: #f7ac8e;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_53c67_row1_col7 {\n",
              "  background-color: #5673e0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_53c67_row1_col8 {\n",
              "  background-color: #5b7ae5;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_53c67_row1_col9 {\n",
              "  background-color: #f7b89c;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_53c67_row2_col1 {\n",
              "  background-color: #f3c7b1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_53c67_row2_col6 {\n",
              "  background-color: #ecd3c5;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_53c67_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >0</th>\n",
              "      <th class=\"col_heading level0 col1\" >1</th>\n",
              "      <th class=\"col_heading level0 col2\" >2</th>\n",
              "      <th class=\"col_heading level0 col3\" >3</th>\n",
              "      <th class=\"col_heading level0 col4\" >4</th>\n",
              "      <th class=\"col_heading level0 col5\" >5</th>\n",
              "      <th class=\"col_heading level0 col6\" >6</th>\n",
              "      <th class=\"col_heading level0 col7\" >7</th>\n",
              "      <th class=\"col_heading level0 col8\" >8</th>\n",
              "      <th class=\"col_heading level0 col9\" >9</th>\n",
              "      <th class=\"col_heading level0 col10\" >10</th>\n",
              "      <th class=\"col_heading level0 col11\" >11</th>\n",
              "      <th class=\"col_heading level0 col12\" >12</th>\n",
              "      <th class=\"col_heading level0 col13\" >13</th>\n",
              "      <th class=\"col_heading level0 col14\" >14</th>\n",
              "      <th class=\"col_heading level0 col15\" >15</th>\n",
              "      <th class=\"col_heading level0 col16\" >16</th>\n",
              "      <th class=\"col_heading level0 col17\" >17</th>\n",
              "      <th class=\"col_heading level0 col18\" >18</th>\n",
              "      <th class=\"col_heading level0 col19\" >19</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >title</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "      <th class=\"blank col11\" >&nbsp;</th>\n",
              "      <th class=\"blank col12\" >&nbsp;</th>\n",
              "      <th class=\"blank col13\" >&nbsp;</th>\n",
              "      <th class=\"blank col14\" >&nbsp;</th>\n",
              "      <th class=\"blank col15\" >&nbsp;</th>\n",
              "      <th class=\"blank col16\" >&nbsp;</th>\n",
              "      <th class=\"blank col17\" >&nbsp;</th>\n",
              "      <th class=\"blank col18\" >&nbsp;</th>\n",
              "      <th class=\"blank col19\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_53c67_level0_row0\" class=\"row_heading level0 row0\" >The Midnight Meat Train</th>\n",
              "      <td id=\"T_53c67_row0_col0\" class=\"data row0 col0\" >3533227.00</td>\n",
              "      <td id=\"T_53c67_row0_col1\" class=\"data row0 col1\" >98.00</td>\n",
              "      <td id=\"T_53c67_row0_col2\" class=\"data row0 col2\" >6.00</td>\n",
              "      <td id=\"T_53c67_row0_col3\" class=\"data row0 col3\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col4\" class=\"data row0 col4\" >16.52</td>\n",
              "      <td id=\"T_53c67_row0_col5\" class=\"data row0 col5\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col6\" class=\"data row0 col6\" >15.08</td>\n",
              "      <td id=\"T_53c67_row0_col7\" class=\"data row0 col7\" >3.04</td>\n",
              "      <td id=\"T_53c67_row0_col8\" class=\"data row0 col8\" >5.67</td>\n",
              "      <td id=\"T_53c67_row0_col9\" class=\"data row0 col9\" >4.00</td>\n",
              "      <td id=\"T_53c67_row0_col10\" class=\"data row0 col10\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col11\" class=\"data row0 col11\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col12\" class=\"data row0 col12\" >1.00</td>\n",
              "      <td id=\"T_53c67_row0_col13\" class=\"data row0 col13\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col14\" class=\"data row0 col14\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col15\" class=\"data row0 col15\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col16\" class=\"data row0 col16\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col17\" class=\"data row0 col17\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col18\" class=\"data row0 col18\" >0.00</td>\n",
              "      <td id=\"T_53c67_row0_col19\" class=\"data row0 col19\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53c67_level0_row1\" class=\"row_heading level0 row1\" >London to Brighton</th>\n",
              "      <td id=\"T_53c67_row1_col0\" class=\"data row1 col0\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col1\" class=\"data row1 col1\" >85.00</td>\n",
              "      <td id=\"T_53c67_row1_col2\" class=\"data row1 col2\" >6.50</td>\n",
              "      <td id=\"T_53c67_row1_col3\" class=\"data row1 col3\" >1.00</td>\n",
              "      <td id=\"T_53c67_row1_col4\" class=\"data row1 col4\" >16.30</td>\n",
              "      <td id=\"T_53c67_row1_col5\" class=\"data row1 col5\" >1.00</td>\n",
              "      <td id=\"T_53c67_row1_col6\" class=\"data row1 col6\" >16.97</td>\n",
              "      <td id=\"T_53c67_row1_col7\" class=\"data row1 col7\" >1.40</td>\n",
              "      <td id=\"T_53c67_row1_col8\" class=\"data row1 col8\" >3.56</td>\n",
              "      <td id=\"T_53c67_row1_col9\" class=\"data row1 col9\" >3.00</td>\n",
              "      <td id=\"T_53c67_row1_col10\" class=\"data row1 col10\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col11\" class=\"data row1 col11\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col12\" class=\"data row1 col12\" >1.00</td>\n",
              "      <td id=\"T_53c67_row1_col13\" class=\"data row1 col13\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col14\" class=\"data row1 col14\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col15\" class=\"data row1 col15\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col16\" class=\"data row1 col16\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col17\" class=\"data row1 col17\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col18\" class=\"data row1 col18\" >0.00</td>\n",
              "      <td id=\"T_53c67_row1_col19\" class=\"data row1 col19\" >0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_53c67_level0_row2\" class=\"row_heading level0 row2\" >The Wash</th>\n",
              "      <td id=\"T_53c67_row2_col0\" class=\"data row2 col0\" >10229331.00</td>\n",
              "      <td id=\"T_53c67_row2_col1\" class=\"data row2 col1\" >93.00</td>\n",
              "      <td id=\"T_53c67_row2_col2\" class=\"data row2 col2\" >5.30</td>\n",
              "      <td id=\"T_53c67_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col4\" class=\"data row2 col4\" >15.76</td>\n",
              "      <td id=\"T_53c67_row2_col5\" class=\"data row2 col5\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col6\" class=\"data row2 col6\" >16.14</td>\n",
              "      <td id=\"T_53c67_row2_col7\" class=\"data row2 col7\" >1.24</td>\n",
              "      <td id=\"T_53c67_row2_col8\" class=\"data row2 col8\" >3.30</td>\n",
              "      <td id=\"T_53c67_row2_col9\" class=\"data row2 col9\" >1.00</td>\n",
              "      <td id=\"T_53c67_row2_col10\" class=\"data row2 col10\" >1.00</td>\n",
              "      <td id=\"T_53c67_row2_col11\" class=\"data row2 col11\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col12\" class=\"data row2 col12\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col13\" class=\"data row2 col13\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col14\" class=\"data row2 col14\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col15\" class=\"data row2 col15\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col16\" class=\"data row2 col16\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col17\" class=\"data row2 col17\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col18\" class=\"data row2 col18\" >0.00</td>\n",
              "      <td id=\"T_53c67_row2_col19\" class=\"data row2 col19\" >0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vXEmb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zO5-RZ0_Dve",
        "outputId": "57a46f10-3393-431e-dd59-e67b943f04e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2402, 940)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERUUYEIoodEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# m = RidgeClassifier(random_state=0)  # multi-label model\n",
        "# %time m.fit(tXEmb, tY)               # fitting to training I/O"
      ],
      "metadata": {
        "id": "BMir8mR8huEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# # this should be the metric used by Kaggle\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros ((n_splits, 20))\n",
        "# best_alphas = []\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     X_train, X_test = tXEmb[train_index], tXEmb[test_index]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     # as at this moment not normalized slightly better (770), but better normalized for further\n",
        "#     # only scaling, non-dummy variables!!! (DIDN'T WORK!!!)\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train[:, :] = scaler.fit_transform(X_train[:, :])\n",
        "#     X_test[:, :] = scaler.transform(X_test[:, :])\n",
        "\n",
        "#     # slightly better and more stable with scaler!\n",
        "#     m = RidgeClassifierCV(alphas = [1, 2, 5, 10, 20, 50, 100, 200, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250], cv = 5)\n",
        "#     m = RidgeClassifier(alpha = 1500, random_state = 0)\n",
        "#     m.fit(X_train, y_train);\n",
        "    \n",
        "#     #best_alphas.append(m.alpha_)\n",
        "\n",
        "#     pred = m.predict(X_test)\n",
        "\n",
        "#     for i in range(20):\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred[:, i], average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))\n",
        "# print(best_alphas)"
      ],
      "metadata": {
        "id": "_QZud5elD9MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Well, non-normalized better but shrinkage is less in non-normalized, so I will try to select variables from normalized with low shrinkage and re-run normalized with selected variables at the low shrinkage"
      ],
      "metadata": {
        "id": "g65tlDHs3qTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection: choosing ones with highest values of coefficient (sine normalized features)"
      ],
      "metadata": {
        "id": "MCMGBBoPBlqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# which classes we want to improve\n",
        "# perf_classes = np.array(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))\n",
        "# np.where(perf_classes < 9.5)"
      ],
      "metadata": {
        "id": "v8ssMtCl5tMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# # this should be the metric used by Kaggle\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros((n_splits, 20))\n",
        "# best_alphas = []\n",
        "\n",
        "# coefs_matrix = np.zeros((5, 20, tXEmb.shape[1]))\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     X_train, X_test = tXEmb[train_index], tXEmb[test_index]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     # as at this moment not normalized slightly better (770), but better normalized for further\n",
        "#     # only scaling, non-dummy variables!!! (DIDN'T WORK!!!)\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train[:, :] = scaler.fit_transform(X_train[:, :])\n",
        "#     X_test[:, :] = scaler.transform(X_test[:, :])\n",
        "\n",
        "#     # slightly better and more stable with scaler!\n",
        "#     m = RidgeClassifierCV(alphas = [10, 20, 50, 100], cv = 5)\n",
        "#     m.fit(X_train, y_train);\n",
        "    \n",
        "#     best_alphas.append(m.alpha_)\n",
        "\n",
        "#     coefs_matrix[j] = m.coef_\n",
        "\n",
        "#     pred = m.predict(X_test)\n",
        "\n",
        "#     for i in range(20):\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred[:, i], average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))\n",
        "# print(best_alphas)"
      ],
      "metadata": {
        "id": "24VbgKcV362c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only relevant targets!!!\n",
        "# abs_avg_coefs = np.abs(coefs_matrix[:, [ 0,  1,  3,  4,  6,  7,  8, 11, 13, 14, 15, 17], :].mean(axis = 0)).sum(axis = 0)"
      ],
      "metadata": {
        "id": "HkxxTpQl76IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so choose only top k with highest values, vary alpha in the cell above!!!!\n",
        "# abs_avg_coefs"
      ],
      "metadata": {
        "id": "lenQ8PZW8t16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected_vars = list(np.where(abs_avg_coefs > 0.17)[0])"
      ],
      "metadata": {
        "id": "kqZSjP4dhNTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tXEmb[train_index][:, selected_vars].shape"
      ],
      "metadata": {
        "id": "hTeN7JEpi8Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best model with selected features"
      ],
      "metadata": {
        "id": "oDiYBs23CHdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# # this should be the metric used by Kaggle\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros((n_splits, 20))\n",
        "# best_alphas = []\n",
        "\n",
        "# coefs_matrix = np.zeros((5, 20, tXEmb[:, selected_vars].shape[1]))\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     # selected vars!!!!\n",
        "#     X_train, X_test = tXEmb[train_index][:, selected_vars], tXEmb[test_index][:, selected_vars]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     # as at this moment not normalized slightly better (770), but better normalized for further\n",
        "#     # only scaling, non-dummy variables!!! (DIDN'T WORK!!!)\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train[:, :] = scaler.fit_transform(X_train[:, :])\n",
        "#     X_test[:, :] = scaler.transform(X_test[:, :])\n",
        "\n",
        "#     # slightly better and more stable with scaler!\n",
        "#     m = RidgeClassifierCV(alphas = [20, 50, 100, 200, 500, 1000, 1500, 2000], cv = 5)\n",
        "#     m = RidgeClassifier(alpha = 500, random_state = 0)\n",
        "#     m.fit(X_train, y_train);\n",
        "    \n",
        "#     #best_alphas.append(m.alpha_)\n",
        "\n",
        "#     coefs_matrix[j] = m.coef_\n",
        "\n",
        "#     pred = m.predict(X_test)\n",
        "\n",
        "#     for i in range(20):\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred[:, i], average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))\n",
        "# print(best_alphas)"
      ],
      "metadata": {
        "id": "0QgciFxvhynT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48BOPS87Z53W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other models (no success, but LGBM was quite close)"
      ],
      "metadata": {
        "id": "A0LUxV60CMEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# this should be the metric used by Kaggle\n",
        "\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros ((n_splits, 20))\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     X_train, X_test = tXEmb[train_index][:, selected_vars], tXEmb[test_index][:, selected_vars]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     for i in range(20):\n",
        "#       m = LGBMClassifier()\n",
        "#       m.fit(X_train, y_train.iloc[:, i]);\n",
        "\n",
        "#       pred = m.predict(X_test)\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred, average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))"
      ],
      "metadata": {
        "id": "0ihaGTePzTvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # best model scores: old bert\n",
        "# [8.42, 8.67, 9.67, 7.94, 8.75, 9.78, 7.58, 9.23, 9.23, 9.92, 9.55, 9.18, 9.63, 9.2, 8.58, 9.28, 9.98, 8.01, 9.65, 9.8]\n",
        "# # best model score: new bert\n",
        "# [8.57, 8.74, 9.68, 8.07, 8.84, 9.78, 7.28, 9.35, 9.26, 9.92, 9.61, 9.41, 9.66, 9.22, 8.61, 9.41, 9.98, 8.0, 9.7, 9.8]"
      ],
      "metadata": {
        "id": "ztPLfXyDsVcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN: Unsuccessful!!!\n",
        "# # cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# # this should be the metric used by Kaggle\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros ((n_splits, 20))\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     X_train, X_test = tXEmb[train_index][:, selected_vars], tXEmb[test_index][:, selected_vars]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     m = KNeighborsClassifier(n_neighbors = 10)\n",
        "#     m.fit(X_train, y_train);\n",
        "\n",
        "#     pred = m.predict_proba(X_test)\n",
        "#     pred_proba = np.zeros((X_test.shape[0], 20))\n",
        "#     for num, _ in enumerate(pred):\n",
        "#         pred_proba[:, num] = pred[num][:, 1]\n",
        "\n",
        "#     pred_final = np.int_(pred_proba > 0.3)\n",
        "\n",
        "#     for i in range(20):\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred_final[:, i], average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))"
      ],
      "metadata": {
        "id": "ekSMK3R3qMYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest no success\n",
        "\n",
        "# # cross-val score based on metric proposed by Gleb Kudryashov: https://discourse.hsemlp.ru/t/moviegenres-evaluation-metric/371/3\n",
        "# # this should be the metric used by Kaggle\n",
        "# n_splits = 5\n",
        "# kf = KFold(n_splits = n_splits)\n",
        "# # 20 classes\n",
        "# score_matrix = np.zeros ((n_splits, 20))\n",
        "\n",
        "# for j, (train_index, test_index) in enumerate(kf.split(tXEmb)):\n",
        "\n",
        "#     X_train, X_test = tXEmb[train_index][:, selected_vars], tXEmb[test_index][:, selected_vars]\n",
        "#     y_train, y_test = tY.iloc[train_index], tY.iloc[test_index]\n",
        "\n",
        "#     m = RandomForestClassifier(n_estimators = 100)\n",
        "#     m.fit(X_train, y_train);\n",
        "\n",
        "#     pred = m.predict_proba(X_test)\n",
        "#     pred_proba = np.zeros((X_test.shape[0], 20))\n",
        "#     for num, _ in enumerate(pred):\n",
        "#         pred_proba[:, num] = pred[num][:, 1]\n",
        "\n",
        "#     pred_final = np.int_(pred_proba > 0.45)\n",
        "\n",
        "#     for i in range(20):\n",
        "#       score_matrix[j, i] = f1_score(y_test.iloc[:, i], pred_final[:, i], average = 'micro')\n",
        "\n",
        "# print('\\n')\n",
        "\n",
        "# print(score_matrix.mean())\n",
        "# print(list(np.round(score_matrix.mean(axis = 1) * 100, 2)))\n",
        "# print(list(np.round(score_matrix.mean(axis = 0) * 10, 2)))"
      ],
      "metadata": {
        "id": "hMDYv5ctyvPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RFMSCGT9VAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final models and making predictions!"
      ],
      "metadata": {
        "id": "ZRKesBnkCfLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# features selected previously!!!!\n",
        "selected_vars = [0, 1, 2, 4, 6, 7, 8, 11, 12, 15, 16, 21, 22, 25, 30, 34, 38, 47, 58, 60, 61, 62, 63, 77, 78, 79, 85, 101, 104, 106, 109, 129, 131, 158,\n",
        " 173, 174, 175, 181, 182, 185, 186, 187, 188, 189, 191, 194, 196, 197, 198, 200, 201, 202, 203, 206, 207, 208, 209, 210, 211, 212, 213, 216, 218, 219, 221, 222, 223, 225, 226, 230,  \n",
        " 231,  234,  236,  237,  238,  239,  240,  242,  243,  244,  245,  248,  249,  250,  251,  252,  253,  254,  255,  256,  257,  258,  260,  261,  262,  266,  267,  269,  270,  272,  275,  276,  277,  280,  283,  284,  288,  290,  291,  293,  295,  298,  299,  300,  301,  302,  304,  305,  306,  308,  310,\n",
        " 311, 313, 315, 316, 317, 318, 320, 326, 327, 328, 329, 330, 333, 334, 335, 336, 339, 340, 342, 344, 345, 347, 348, 349, 350, 352, 353, 354, 355, 356, 358, 360, 363, 364, 367, 370, 371, 372, 373, 374, 375, 379, 382, 383, 384, 385, 386, 388, 389, 390, 392, 393, 394, 395,\n",
        " 397, 398, 399, 400, 402, 403, 405, 406, 407, 408, 411, 412, 413, 415, 418, 420, 422, 425, 426, 430, 432, 434, 435, 436, 437, 438, 439, 441, 442, 446, 448, 449, 450, 451, 452, 454, 455, 456, 458, 460, 462, 463, 464, 469, 470, 471, 472, 474, 476, 478, 481, 482, 483,\n",
        " 488, 489, 490, 493, 494, 495, 497, 499, 500, 501, 502, 503, 504, 505, 507, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 521, 523, 525, 529, 530, 531, 532, 533, 535, 536, 537, 538, 540, 541, 542, 543, 546, 547, 548, 552, 554, 557, 558, 560, 561, 565, 566, 567, 569,\n",
        " 570, 571, 573, 574, 575, 576, 577, 579, 580, 581, 582, 586, 587, 588, 590, 591, 593, 594, 595, 597, 598, 599, 600, 603, 604, 605, 607, 608, 609, 615, 616, 618, 620, 622, 625, 626, 627, 628, 629, 630, 634, 635, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 651, 655, 656, 657,\n",
        " 658, 659, 660, 661, 662, 663, 664, 665, 667, 668, 669, 670, 671, 672, 674, 675, 676, 677, 678, 679, 681, 683, 684, 686, 688, 689, 690, 691, 692, 693, 696, 700, 701, 704, 705, 710, 711, 712, 713, 715, 716, 717, 718, 719, 721, 722, 723, 724, 727, 730, 732, 734, 735, 736, 738, 739, 740, 743, 746, 749,\n",
        " 750, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 779, 780, 781, 783, 784, 786, 788, 789, 790, 793, 794, 795, 796, 799, 800, 801, 804, 805, 806, 807, 808, 809, 811, 812, 813, \n",
        " 814, 815, 816, 818, 820, 821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 839, 840, 841, 842, 843, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 857, 858, 859, 860, 862, 865, 866, 867, 869, 870, 872, 873, 874, 877, 878, 880, 881, 882, 883, 885, 886, 887, 891,\n",
        " 892, 893, 897, 898, 899, 900, 904, 907, 909, 910, 911, 914, 915, 916, 917, 918, 919, 920, 921, 922, 924, 925, 926, 927, 929, 930, 931, 932, 934, 935, 936, 938]"
      ],
      "metadata": {
        "id": "asC8Q5lWClh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current final model\n",
        "X_train = tXEmb[:, selected_vars]\n",
        "y_train = tY\n",
        "X_test = vXEmb[:, selected_vars]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# for model with selected features best alpha is 650!!!!\n",
        "#m = RidgeClassifierCV(alphas = [5, 10, 15, 20, 30, 50, 75, 100, 200, 350, 500, 650, 750, 900, 1000, 1250, 1500], cv = 5)\n",
        "# using alpha 650\n",
        "m = RidgeClassifier(alpha = 650, random_state = 0)\n",
        "m.fit(X_train, y_train)\n",
        "\n",
        "#m.alpha_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjH1Rit68WEv",
        "outputId": "2b8564f1-ce94-4668-9cb6-579adadea060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RidgeClassifier(alpha=650, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pY = pd.DataFrame(m.predict(X_test), index=range(len(vX)), columns=YCols)  # Don't shuffle observations. Keep original order with index 0,1,2,...\n",
        "pd.DataFrame(pY.values[:10,:], index=vX.title[:10], columns=YCols).style.background_gradient(cmap='coolwarm', axis=1)\n",
        "\n",
        "# UNCOMMENT THIS LINE TO SAVE FILE!!!\n",
        "#pY.reset_index().rename(columns={'index':'id'}).to_csv('All_feature_extraction_feature_selection_new_bert_normalized1_less_features1.csv', index=False)  "
      ],
      "metadata": {
        "id": "JVaMWBvWY_zF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "13cd6074-f2c2-450e-fed1-cef3aedcf531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f2ae0c05650>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_666c3_row0_col0, #T_666c3_row0_col1, #T_666c3_row0_col2, #T_666c3_row0_col3, #T_666c3_row0_col4, #T_666c3_row0_col5, #T_666c3_row0_col7, #T_666c3_row0_col8, #T_666c3_row0_col9, #T_666c3_row0_col10, #T_666c3_row0_col11, #T_666c3_row0_col12, #T_666c3_row0_col13, #T_666c3_row0_col14, #T_666c3_row0_col15, #T_666c3_row0_col16, #T_666c3_row0_col17, #T_666c3_row0_col18, #T_666c3_row0_col19, #T_666c3_row1_col0, #T_666c3_row1_col1, #T_666c3_row1_col2, #T_666c3_row1_col3, #T_666c3_row1_col5, #T_666c3_row1_col7, #T_666c3_row1_col8, #T_666c3_row1_col9, #T_666c3_row1_col10, #T_666c3_row1_col11, #T_666c3_row1_col12, #T_666c3_row1_col13, #T_666c3_row1_col14, #T_666c3_row1_col15, #T_666c3_row1_col16, #T_666c3_row1_col17, #T_666c3_row1_col18, #T_666c3_row1_col19, #T_666c3_row2_col0, #T_666c3_row2_col1, #T_666c3_row2_col2, #T_666c3_row2_col3, #T_666c3_row2_col4, #T_666c3_row2_col5, #T_666c3_row2_col7, #T_666c3_row2_col8, #T_666c3_row2_col9, #T_666c3_row2_col10, #T_666c3_row2_col11, #T_666c3_row2_col12, #T_666c3_row2_col13, #T_666c3_row2_col14, #T_666c3_row2_col15, #T_666c3_row2_col16, #T_666c3_row2_col17, #T_666c3_row2_col18, #T_666c3_row2_col19, #T_666c3_row3_col0, #T_666c3_row3_col1, #T_666c3_row3_col2, #T_666c3_row3_col4, #T_666c3_row3_col5, #T_666c3_row3_col6, #T_666c3_row3_col7, #T_666c3_row3_col8, #T_666c3_row3_col9, #T_666c3_row3_col10, #T_666c3_row3_col11, #T_666c3_row3_col12, #T_666c3_row3_col13, #T_666c3_row3_col14, #T_666c3_row3_col15, #T_666c3_row3_col16, #T_666c3_row3_col17, #T_666c3_row3_col18, #T_666c3_row3_col19, #T_666c3_row4_col0, #T_666c3_row4_col1, #T_666c3_row4_col2, #T_666c3_row4_col4, #T_666c3_row4_col5, #T_666c3_row4_col6, #T_666c3_row4_col8, #T_666c3_row4_col9, #T_666c3_row4_col10, #T_666c3_row4_col11, #T_666c3_row4_col12, #T_666c3_row4_col13, #T_666c3_row4_col14, #T_666c3_row4_col15, #T_666c3_row4_col16, #T_666c3_row4_col17, #T_666c3_row4_col18, #T_666c3_row4_col19, #T_666c3_row5_col0, #T_666c3_row5_col1, #T_666c3_row5_col2, #T_666c3_row5_col3, #T_666c3_row5_col5, #T_666c3_row5_col6, #T_666c3_row5_col7, #T_666c3_row5_col8, #T_666c3_row5_col9, #T_666c3_row5_col10, #T_666c3_row5_col12, #T_666c3_row5_col13, #T_666c3_row5_col14, #T_666c3_row5_col15, #T_666c3_row5_col16, #T_666c3_row5_col18, #T_666c3_row5_col19, #T_666c3_row6_col0, #T_666c3_row6_col1, #T_666c3_row6_col2, #T_666c3_row6_col4, #T_666c3_row6_col5, #T_666c3_row6_col6, #T_666c3_row6_col7, #T_666c3_row6_col8, #T_666c3_row6_col9, #T_666c3_row6_col10, #T_666c3_row6_col11, #T_666c3_row6_col12, #T_666c3_row6_col13, #T_666c3_row6_col14, #T_666c3_row6_col15, #T_666c3_row6_col16, #T_666c3_row6_col17, #T_666c3_row6_col18, #T_666c3_row6_col19, #T_666c3_row7_col0, #T_666c3_row7_col1, #T_666c3_row7_col2, #T_666c3_row7_col4, #T_666c3_row7_col5, #T_666c3_row7_col6, #T_666c3_row7_col7, #T_666c3_row7_col8, #T_666c3_row7_col9, #T_666c3_row7_col10, #T_666c3_row7_col11, #T_666c3_row7_col12, #T_666c3_row7_col13, #T_666c3_row7_col14, #T_666c3_row7_col15, #T_666c3_row7_col16, #T_666c3_row7_col17, #T_666c3_row7_col18, #T_666c3_row7_col19, #T_666c3_row8_col0, #T_666c3_row8_col1, #T_666c3_row8_col2, #T_666c3_row8_col4, #T_666c3_row8_col5, #T_666c3_row8_col6, #T_666c3_row8_col8, #T_666c3_row8_col9, #T_666c3_row8_col10, #T_666c3_row8_col11, #T_666c3_row8_col12, #T_666c3_row8_col13, #T_666c3_row8_col14, #T_666c3_row8_col15, #T_666c3_row8_col16, #T_666c3_row8_col17, #T_666c3_row8_col18, #T_666c3_row8_col19, #T_666c3_row9_col0, #T_666c3_row9_col1, #T_666c3_row9_col2, #T_666c3_row9_col3, #T_666c3_row9_col4, #T_666c3_row9_col5, #T_666c3_row9_col6, #T_666c3_row9_col7, #T_666c3_row9_col8, #T_666c3_row9_col9, #T_666c3_row9_col10, #T_666c3_row9_col12, #T_666c3_row9_col13, #T_666c3_row9_col14, #T_666c3_row9_col15, #T_666c3_row9_col16, #T_666c3_row9_col17, #T_666c3_row9_col18, #T_666c3_row9_col19 {\n",
              "  background-color: #3b4cc0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_666c3_row0_col6, #T_666c3_row1_col4, #T_666c3_row1_col6, #T_666c3_row2_col6, #T_666c3_row3_col3, #T_666c3_row4_col3, #T_666c3_row4_col7, #T_666c3_row5_col4, #T_666c3_row5_col11, #T_666c3_row5_col17, #T_666c3_row6_col3, #T_666c3_row7_col3, #T_666c3_row8_col3, #T_666c3_row8_col7, #T_666c3_row9_col11 {\n",
              "  background-color: #b40426;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_666c3_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Action</th>\n",
              "      <th class=\"col_heading level0 col1\" >Adventure</th>\n",
              "      <th class=\"col_heading level0 col2\" >Animation</th>\n",
              "      <th class=\"col_heading level0 col3\" >Comedy</th>\n",
              "      <th class=\"col_heading level0 col4\" >Crime</th>\n",
              "      <th class=\"col_heading level0 col5\" >Documentary</th>\n",
              "      <th class=\"col_heading level0 col6\" >Drama</th>\n",
              "      <th class=\"col_heading level0 col7\" >Family</th>\n",
              "      <th class=\"col_heading level0 col8\" >Fantasy</th>\n",
              "      <th class=\"col_heading level0 col9\" >Foreign</th>\n",
              "      <th class=\"col_heading level0 col10\" >History</th>\n",
              "      <th class=\"col_heading level0 col11\" >Horror</th>\n",
              "      <th class=\"col_heading level0 col12\" >Music</th>\n",
              "      <th class=\"col_heading level0 col13\" >Mystery</th>\n",
              "      <th class=\"col_heading level0 col14\" >Romance</th>\n",
              "      <th class=\"col_heading level0 col15\" >Science Fiction</th>\n",
              "      <th class=\"col_heading level0 col16\" >TV Movie</th>\n",
              "      <th class=\"col_heading level0 col17\" >Thriller</th>\n",
              "      <th class=\"col_heading level0 col18\" >War</th>\n",
              "      <th class=\"col_heading level0 col19\" >Western</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >title</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "      <th class=\"blank col9\" >&nbsp;</th>\n",
              "      <th class=\"blank col10\" >&nbsp;</th>\n",
              "      <th class=\"blank col11\" >&nbsp;</th>\n",
              "      <th class=\"blank col12\" >&nbsp;</th>\n",
              "      <th class=\"blank col13\" >&nbsp;</th>\n",
              "      <th class=\"blank col14\" >&nbsp;</th>\n",
              "      <th class=\"blank col15\" >&nbsp;</th>\n",
              "      <th class=\"blank col16\" >&nbsp;</th>\n",
              "      <th class=\"blank col17\" >&nbsp;</th>\n",
              "      <th class=\"blank col18\" >&nbsp;</th>\n",
              "      <th class=\"blank col19\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row0\" class=\"row_heading level0 row0\" >Hachi: A Dog's Tale</th>\n",
              "      <td id=\"T_666c3_row0_col0\" class=\"data row0 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col1\" class=\"data row0 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col2\" class=\"data row0 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col3\" class=\"data row0 col3\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col4\" class=\"data row0 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col6\" class=\"data row0 col6\" >1</td>\n",
              "      <td id=\"T_666c3_row0_col7\" class=\"data row0 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col10\" class=\"data row0 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col11\" class=\"data row0 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col12\" class=\"data row0 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col13\" class=\"data row0 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col14\" class=\"data row0 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col15\" class=\"data row0 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col16\" class=\"data row0 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col17\" class=\"data row0 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col18\" class=\"data row0 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row0_col19\" class=\"data row0 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row1\" class=\"row_heading level0 row1\" >The Sting</th>\n",
              "      <td id=\"T_666c3_row1_col0\" class=\"data row1 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col3\" class=\"data row1 col3\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col4\" class=\"data row1 col4\" >1</td>\n",
              "      <td id=\"T_666c3_row1_col5\" class=\"data row1 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col6\" class=\"data row1 col6\" >1</td>\n",
              "      <td id=\"T_666c3_row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col10\" class=\"data row1 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col11\" class=\"data row1 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col12\" class=\"data row1 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col13\" class=\"data row1 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col14\" class=\"data row1 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col15\" class=\"data row1 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col16\" class=\"data row1 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col17\" class=\"data row1 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col18\" class=\"data row1 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row2\" class=\"row_heading level0 row2\" >The Book of Life</th>\n",
              "      <td id=\"T_666c3_row2_col0\" class=\"data row2 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col1\" class=\"data row2 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col3\" class=\"data row2 col3\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col4\" class=\"data row2 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col5\" class=\"data row2 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col6\" class=\"data row2 col6\" >1</td>\n",
              "      <td id=\"T_666c3_row2_col7\" class=\"data row2 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col8\" class=\"data row2 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col10\" class=\"data row2 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col11\" class=\"data row2 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col12\" class=\"data row2 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col13\" class=\"data row2 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col14\" class=\"data row2 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col15\" class=\"data row2 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col16\" class=\"data row2 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col17\" class=\"data row2 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col18\" class=\"data row2 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row2_col19\" class=\"data row2 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row3\" class=\"row_heading level0 row3\" >Dude, Where‚Äôs My Car?</th>\n",
              "      <td id=\"T_666c3_row3_col0\" class=\"data row3 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col1\" class=\"data row3 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col2\" class=\"data row3 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col3\" class=\"data row3 col3\" >1</td>\n",
              "      <td id=\"T_666c3_row3_col4\" class=\"data row3 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col6\" class=\"data row3 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col7\" class=\"data row3 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col8\" class=\"data row3 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col10\" class=\"data row3 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col11\" class=\"data row3 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col12\" class=\"data row3 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col13\" class=\"data row3 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col14\" class=\"data row3 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col15\" class=\"data row3 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col16\" class=\"data row3 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col17\" class=\"data row3 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col18\" class=\"data row3 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row4\" class=\"row_heading level0 row4\" >Bolt</th>\n",
              "      <td id=\"T_666c3_row4_col0\" class=\"data row4 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col1\" class=\"data row4 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col2\" class=\"data row4 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col3\" class=\"data row4 col3\" >1</td>\n",
              "      <td id=\"T_666c3_row4_col4\" class=\"data row4 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col5\" class=\"data row4 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col6\" class=\"data row4 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col7\" class=\"data row4 col7\" >1</td>\n",
              "      <td id=\"T_666c3_row4_col8\" class=\"data row4 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col10\" class=\"data row4 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col11\" class=\"data row4 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col12\" class=\"data row4 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col13\" class=\"data row4 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col14\" class=\"data row4 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col15\" class=\"data row4 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col16\" class=\"data row4 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col17\" class=\"data row4 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col18\" class=\"data row4 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row5\" class=\"row_heading level0 row5\" >Saw IV</th>\n",
              "      <td id=\"T_666c3_row5_col0\" class=\"data row5 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col1\" class=\"data row5 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col2\" class=\"data row5 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col3\" class=\"data row5 col3\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col4\" class=\"data row5 col4\" >1</td>\n",
              "      <td id=\"T_666c3_row5_col5\" class=\"data row5 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col6\" class=\"data row5 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col7\" class=\"data row5 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col8\" class=\"data row5 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col9\" class=\"data row5 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col10\" class=\"data row5 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col11\" class=\"data row5 col11\" >1</td>\n",
              "      <td id=\"T_666c3_row5_col12\" class=\"data row5 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col13\" class=\"data row5 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col14\" class=\"data row5 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col15\" class=\"data row5 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col16\" class=\"data row5 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col17\" class=\"data row5 col17\" >1</td>\n",
              "      <td id=\"T_666c3_row5_col18\" class=\"data row5 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row5_col19\" class=\"data row5 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row6\" class=\"row_heading level0 row6\" >Hall Pass</th>\n",
              "      <td id=\"T_666c3_row6_col0\" class=\"data row6 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col1\" class=\"data row6 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col2\" class=\"data row6 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col3\" class=\"data row6 col3\" >1</td>\n",
              "      <td id=\"T_666c3_row6_col4\" class=\"data row6 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col5\" class=\"data row6 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col6\" class=\"data row6 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col7\" class=\"data row6 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col8\" class=\"data row6 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col9\" class=\"data row6 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col10\" class=\"data row6 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col11\" class=\"data row6 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col12\" class=\"data row6 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col13\" class=\"data row6 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col14\" class=\"data row6 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col15\" class=\"data row6 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col16\" class=\"data row6 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col17\" class=\"data row6 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col18\" class=\"data row6 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row6_col19\" class=\"data row6 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row7\" class=\"row_heading level0 row7\" >The Ladies Man</th>\n",
              "      <td id=\"T_666c3_row7_col0\" class=\"data row7 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col1\" class=\"data row7 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col2\" class=\"data row7 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col3\" class=\"data row7 col3\" >1</td>\n",
              "      <td id=\"T_666c3_row7_col4\" class=\"data row7 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col5\" class=\"data row7 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col6\" class=\"data row7 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col7\" class=\"data row7 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col8\" class=\"data row7 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col9\" class=\"data row7 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col10\" class=\"data row7 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col11\" class=\"data row7 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col12\" class=\"data row7 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col13\" class=\"data row7 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col14\" class=\"data row7 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col15\" class=\"data row7 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col16\" class=\"data row7 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col17\" class=\"data row7 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col18\" class=\"data row7 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row7_col19\" class=\"data row7 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row8\" class=\"row_heading level0 row8\" >Jingle All the Way</th>\n",
              "      <td id=\"T_666c3_row8_col0\" class=\"data row8 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col1\" class=\"data row8 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col2\" class=\"data row8 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col3\" class=\"data row8 col3\" >1</td>\n",
              "      <td id=\"T_666c3_row8_col4\" class=\"data row8 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col5\" class=\"data row8 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col6\" class=\"data row8 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col7\" class=\"data row8 col7\" >1</td>\n",
              "      <td id=\"T_666c3_row8_col8\" class=\"data row8 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col9\" class=\"data row8 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col10\" class=\"data row8 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col11\" class=\"data row8 col11\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col12\" class=\"data row8 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col13\" class=\"data row8 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col14\" class=\"data row8 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col15\" class=\"data row8 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col16\" class=\"data row8 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col17\" class=\"data row8 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col18\" class=\"data row8 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row8_col19\" class=\"data row8 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_666c3_level0_row9\" class=\"row_heading level0 row9\" >Abraham Lincoln: Vampire Hunter</th>\n",
              "      <td id=\"T_666c3_row9_col0\" class=\"data row9 col0\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col1\" class=\"data row9 col1\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col2\" class=\"data row9 col2\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col3\" class=\"data row9 col3\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col4\" class=\"data row9 col4\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col5\" class=\"data row9 col5\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col6\" class=\"data row9 col6\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col7\" class=\"data row9 col7\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col8\" class=\"data row9 col8\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col9\" class=\"data row9 col9\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col10\" class=\"data row9 col10\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col11\" class=\"data row9 col11\" >1</td>\n",
              "      <td id=\"T_666c3_row9_col12\" class=\"data row9 col12\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col13\" class=\"data row9 col13\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col14\" class=\"data row9 col14\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col15\" class=\"data row9 col15\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col16\" class=\"data row9 col16\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col17\" class=\"data row9 col17\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col18\" class=\"data row9 col18\" >0</td>\n",
              "      <td id=\"T_666c3_row9_col19\" class=\"data row9 col19\" >0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **References:**"
      ],
      "metadata": {
        "id": "pzBsjCvS_kEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Remember to cite your sources here as well! At the least, your textbook should be cited. Google Scholar allows you to effortlessly copy/paste an APA citation format for books and publications. Also cite StackOverflow, package documentation, and other meaningful internet resources to help your peers learn from these (and to avoid plagiarism claims)."
      ],
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ],
      "metadata": {
        "id": "DoF2GoB_QGw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ],
      "metadata": {
        "id": "bD1sdgYbNWQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a6f931d-32c3-469f-cdbb-1498e9777f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime is 42 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## around 63 seconds on GPU. Note that this code should generate 0.91885 public score!!!"
      ],
      "metadata": {
        "id": "Lew6qS7JIoqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note that currently this notebook is not connected to GPU. Switch to GPU, so that the notebook will run faster!"
      ],
      "metadata": {
        "id": "gqrUj8oSj70N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí°**Starter Ideas**"
      ],
      "metadata": {
        "id": "5X8HxjZdXJQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tune model hyperparameters\n",
        "1. Try to linear and non-linear feature normalization: shift/scale, log, divide features by features (investigate scatterplot matrix)\n",
        "1. Try higher order feature interactions and polynomial features for the original numeric features. Then identify key features or select key principal components. The final model can be trained on a larger or even full training sample. You can use [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the feature set\n",
        "1. Do a thorough EDA: look for feature augmentations that result in suitable decision boundaries between pairs of classes (for example, linear boundary for linear models).\n",
        "1. Evaluate predictions and focus on poorly predicted \"groups\":\n",
        "  1. Strongest misclassifications. E.g. the model is very confident about the wrong label\n",
        "  1. Evaluate predictions near decision boundaries. Is there a way to separate mixed points near the boundary by, perhaps, introducing additional dimensions or interactions?\n",
        "1. Do scatter plots show piecewise linear shape? Can a separate linear model be used on each support, or can the pattern be linearized via transformations?\n",
        "1. Clean up textual fields to remove uninformative text. For example, you can use [`json`](https://docs.python.org/3/library/json.html) and [`re`](https://docs.python.org/3/library/re.html) to retrieve just keywords from the lists of [JSON](https://en.wikipedia.org/wiki/JSON) keywords. This may speed up embeddings and lower noise in output coefficients.\n",
        "   1. Example: `'[{\"id\": 907, \"name\": \"japanese\"}, ...'` $\\to$ `'japanese, loyalty, friendship, ...'`\n",
        "1. Fill input `NA`'s with more suitable statistic (for example, column or group mean or median)\n",
        "1. Replaces extreme numeric values (such as zeros) with some statistic (such as mean/median) or a modeled value or `NA` (if the predictive model can handle `NA` inputs)\n",
        "1. Consider embedding other textual fields, if they appear to relate to genres. For example, some countries or companies may focus on documentaries or on action films\n",
        "1. Try to find a more [suitable SBERT embedding](https://www.sbert.net/docs/pretrained_models.html\n",
        "1. Consider embedding \"important\" (for prediction) textual fields separately and concatenating or summing their vectors.\n",
        ") (small, fast, trained on related text)\n",
        "1. Learn about [TMDB](https://www.themoviedb.org/) dataset and [related models](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=tmdb+machine+learning+model&btnG=).\n"
      ],
      "metadata": {
        "id": "QX9feyDlpej_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Ecq5_HyGtnt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}