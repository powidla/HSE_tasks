{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8+/VaPipSr5PgQRkbKqo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/powidla/HSE_tasks/blob/main/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы научить алгоритм преодолевать лабиринт с помощью машинного обучения, можно использовать технику обучения с подкреплением, например Q-обучение. Обучение с подкреплением хорошо подходит для обучения агентов навигации в среде и принятию решений на основе проб и ошибок. Вот пошаговое руководство по обучению алгоритма с помощью Q-обучения:\n",
        "\n",
        "1. Определите проблему: Определите характеристики и правила вашего лабиринта. Укажите начальную точку, местоположение цели, стены и возможные действия, которые может предпринять агент (например, двигаться вверх, вниз, влево, вправо).\n",
        "\n",
        "2. Настройте пространство состояний: Определите представление лабиринта в виде сетки, где каждая ячейка может быть либо пустой, либо заблокированной стеной. Присвойте уникальные идентификаторы каждой ячейке, чтобы создать пространство состояний.\n",
        "\n",
        "3. Инициализируйте Q-таблицу: Создайте Q-таблицу для хранения значений действий для каждой пары \"состояние-действие\". Q-таблица инициализируется нулями или случайными значениями.\n",
        "\n",
        "4. Определите гиперпараметры: Определите скорость обучения (α), коэффициент дисконтирования (γ) и скорость исследования (ε). Эти гиперпараметры контролируют компромисс между эксплуатацией (выбор действий на основе полученных знаний) и исследованием (опробование новых действий).\n",
        "\n",
        "5. Цикл обучения: Итерация по эпизодам для обучения агента. Каждый эпизод состоит из следующих шагов:\n",
        "\n",
        "   a. Сброс среды: Поместите агента в начальную точку лабиринта.\n",
        "   \n",
        "   b. Выбрать действие: Используйте ε-жадную политику для выбора действия. С вероятностью ε выберите случайное действие; в противном случае выберите действие с наибольшим значением Q для текущего состояния.\n",
        "   \n",
        "   c. Выполнить действие: Переместите агента в соответствии с выбранным действием и наблюдайте за следующим состоянием и вознаграждением.\n",
        "   \n",
        "   d. Обновить Q-таблицу: Используйте правило обновления Q-обучения для обновления Q-значения для текущей пары \"состояние-действие\" на основе наблюдаемой награды и максимального Q-значения следующего состояния.\n",
        "   \n",
        "   e. Повторяйте шаги b-d, пока агент не достигнет цели или не застрянет в цикле.\n",
        "   \n",
        "   f. Обновить окружение: Если агент достиг цели, отметьте эпизод как успешный. В противном случае отметьте его как неудачный.\n",
        "   \n",
        "   g. Снизить скорость разведки: Снижайте скорость исследования ε со временем, чтобы стимулировать эксплуатацию.\n",
        "   \n",
        "   h. Повторяйте шаги a-g, пока значения Q не сойдутся или не будет достигнуто максимальное количество эпизодов.\n",
        "\n",
        "6. Тестирование: После обучения оцените производительность обученного агента, позволив ему пройти лабиринт, используя выученную политику. Используйте самые высокие Q-значения для определения действий, которые необходимо предпринять в каждом состоянии.\n",
        "\n",
        "7. Тонкая настройка: Если работа агента неудовлетворительна, скорректируйте гиперпараметры или измените структуру вознаграждения, чтобы улучшить процесс обучения.\n",
        "\n",
        "8. Повторяйте шаги 5-7 до тех пор, пока агент не будет последовательно эффективно решать задачу лабиринта."
      ],
      "metadata": {
        "id": "vvjBinkBZO8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Без визуализации"
      ],
      "metadata": {
        "id": "ZmaUE84hXwHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Задаем лабиринт как матрицу\n",
        "labyrinth = np.array([\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 1, 0, 1, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "# Определим стартовое положение агента и конечное положение которое будет считаться его конечной целью\n",
        "start_pos = (0, 0)\n",
        "goal_pos = (4, 4)\n",
        "\n",
        "# Определим возможные варианты действия агента\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Инициализируем Q-таблицу\n",
        "q_table = np.zeros((labyrinth.shape[0], labyrinth.shape[1], len(actions)))\n",
        "\n",
        "# Параметры обучения\n",
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "exploration_rate = 0.8\n",
        "max_episodes = 1000\n",
        "\n",
        "# Цикл обучения агента\n",
        "for episode in range(max_episodes):\n",
        "    # Положение агента\n",
        "    current_pos = start_pos\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Условие остановки цикла\n",
        "        if np.random.uniform() < exploration_rate:\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            action = actions[np.argmax(q_table[current_pos])]\n",
        "\n",
        "        # Подвигаем агента\n",
        "        if action == 'up':\n",
        "            next_pos = (current_pos[0] - 1, current_pos[1])\n",
        "        elif action == 'down':\n",
        "            next_pos = (current_pos[0] + 1, current_pos[1])\n",
        "        elif action == 'left':\n",
        "            next_pos = (current_pos[0], current_pos[1] - 1)\n",
        "        elif action == 'right':\n",
        "            next_pos = (current_pos[0], current_pos[1] + 1)\n",
        "\n",
        "        if next_pos == goal_pos:\n",
        "            reward = 1  # Успех\n",
        "            done = True\n",
        "        elif next_pos[0] < 0 or next_pos[0] >= labyrinth.shape[0] or next_pos[1] < 0 or next_pos[1] >= labyrinth.shape[1] or labyrinth[next_pos] == 1:\n",
        "            reward = -1  # Врезался в стену\n",
        "            next_pos = current_pos  # Остался на метсе\n",
        "        else:\n",
        "            reward = 0  # Пустая клетка\n",
        "\n",
        "        # Обновление таблицы\n",
        "        q_table[current_pos + (actions.index(action),)] += learning_rate * \\\n",
        "            (reward + discount_factor * np.max(q_table[next_pos]) - q_table[current_pos + (actions.index(action),)])\n",
        "\n",
        "        current_pos = next_pos\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX8dEegJjt5n",
        "outputId": "4386eff8-9ccd-45f5-c3c1-2b550ea85501"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent reached the goal!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ПРоверка работы\n",
        "%%time\n",
        "current_pos = start_pos\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = actions[np.argmax(q_table[current_pos])]\n",
        "    if action == 'up':\n",
        "        next_pos = (current_pos[0] - 1, current_pos[1])\n",
        "    elif action == 'down':\n",
        "        next_pos = (current_pos[0] + 1, current_pos[1])\n",
        "    elif action == 'left':\n",
        "        next_pos = (current_pos[0], current_pos[1] - 1)\n",
        "    elif action == 'right':\n",
        "        next_pos = (current_pos[0], current_pos[1] + 1)\n",
        "\n",
        "    if next_pos == goal_pos:\n",
        "        done = True\n",
        "\n",
        "    current_pos = next_pos\n",
        "\n",
        "print(\"Agent reached the goal!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH_TNS2uZUMp",
        "outputId": "7d950142-eb34-43fe-dfe9-5de3765c6017"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent reached the goal!\n",
            "CPU times: user 133 µs, sys: 0 ns, total: 133 µs\n",
            "Wall time: 137 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "leCtUAdJaARo"
      }
    }
  ]
}