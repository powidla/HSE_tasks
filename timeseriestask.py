# -*- coding: utf-8 -*-
"""TimeSeriesTask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ysst8kadHI_XQfLRuLXeVIkJyOv9lJ8
"""

!pip install pystan==2.19.1.1
!pip install fbprophet

"""# Подключение библиотек"""

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
# from fbprophet import Prophet

"""# Подгрузка данных"""

def load_data(file_path):
    data = pd.read_csv(file_path)
    # Дополнительные операции с данными
    return data

"""# Функция для предсказания"""

def make_forecast(data, model_type, forecast_period):
    # Дополнительные операции по подготовке данных и выбору модели
    p = 3
    d = 4
    q = 1 
    P = 2 
    D = 3 
    Q = 2 
    m =1
    if model_type == 'ARIMA':
        model = ARIMA(data, order=(p, d, q))
        forecast = model.fit().forecast(steps=forecast_period)
    elif model_type == 'SARIMA':
        model = SARIMAX(data, order=(p, d, q), seasonal_order=(P, D, Q, m))
        forecast = model.fit().forecast(steps=forecast_period)
    elif model_type == 'Prophet':
        model = Prophet()
        model.fit(data)
        future = model.make_future_dataframe(periods=forecast_period)
        forecast = model.predict(future)
        forecast = forecast.tail(forecast_period)['yhat'].values
    else:
        raise ValueError('Invalid model type')

    return forecast

"""# Визуализация"""

def plot_forecast(data, forecast):
    plt.plot(data, label='Original Data')
    plt.plot(range(len(data), len(data) + len(forecast)), forecast, label='Forecast')
    plt.xlabel('Time')
    plt.ylabel('Value')
    plt.legend()
    plt.show()

"""# Сохранение резульатата"""

def save_forecast(forecast, file_path):
    forecast_df = pd.DataFrame({'Forecast': forecast})
    forecast_df.to_csv(file_path, index=False)

"""# Генерация данных"""

import pandas as pd
from datetime import datetime, timedelta

# Create a list of dates starting from a specific date
start_date = datetime(2023, 1, 1)
dates = [start_date + timedelta(days=i) for i in range(20)]

# Create a list of values
values = [10, 15, 12, 18, 20, 22, 23, 19, 22, 23, 23, 25, 26, 24, 23, 22, 22, 23, 23, 25]  # Add your 20 values here

# Create a DataFrame with the date and value columns
data = pd.DataFrame({'Date': dates, 'Value': values})

# Save DataFrame to CSV file
data.to_csv('data.csv', index=False)

data

"""# Выбор из моделей"""

import pandas as pd
from statsmodels.tsa.ar_model import AutoReg
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# Загрузка данных из файла
def load_data(file_path):
    data = pd.read_csv(file_path)
    return data

# Функция для оценки AR модели
def evaluate_ar_model(data):
    model = AutoReg(data, lags=1)
    model_fit = model.fit()
    forecast = model_fit.predict(start=len(data), end=len(data)+1)
    return forecast

# Функция для оценки ARIMA модели
def evaluate_arima_model(data):
    model = ARIMA(data, order=(1, 0, 0))
    model_fit = model.fit()
    forecast = model_fit.forecast(steps=1)
    return forecast

# Функция для оценки SARIMA модели
def evaluate_sarima_model(data):
    model = SARIMAX(data, order=(1, 0, 0), seasonal_order=(0, 0, 0, 0))
    model_fit = model.fit()
    forecast = model_fit.forecast(steps=1)
    return forecast

# Загрузка данных
data = load_data('data.csv')
data_values = data['Value']

# Инициализация переменных для хранения наилучшей модели и оценки качества
best_model = None
best_score = float('inf')

# Оценка AR модели
ar_forecast = evaluate_ar_model(data_values)

data_values[7:9]

ar_forecast

ar_score = mean_squared_error(data_values[1:3], ar_forecast)

# Сохранение AR модели, если она лучше текущей наилучшей модели
if ar_score < best_score:
    best_model = 'AR'
    best_score = ar_score

# Оценка ARIMA модели
arima_forecast = evaluate_arima_model(data_values)

arima_forecast

arima_score = mean_squared_error(data_values[8:9], arima_forecast)

# Сохранение ARIMA модели, если она лучше текущей наилучшей модели
if arima_score < best_score:
    best_model = 'ARIMA'
    best_score = arima_score

# Оценка SARIMA модели
sarima_forecast = evaluate_sarima_model(data_values)
sarima_score = mean_squared_error(data_values[8:9], sarima_forecast)

# Сохранение SARIMA модели, если она лучше текущей наилучшей модели
if sarima_score < best_score:
    best_model = 'SARIMA'
    best_score = sarima_score

# Вывод наилучшей модели и оценки качества
print('Best model:', best_model)
print('Best score:', best_score)

# Plotting actual data
plt.plot(data_values, label='Actual')

# Plotting AR forecast
# plt.plot(data_values[1:3], ar_forecast, marker='o', color='red', label='AR Forecast')

# # Plotting ARIMA forecast
# plt.plot(data_values[1:2], arima_forecast, marker='o', color='green', label='ARIMA Forecast')

# # Plotting SARIMA forecast
# plt.plot(data_values[1:2], sarima_forecast, marker='o', color='blue', label='SARIMA Forecast')

plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()

"""# Применим к датасету

* https://www.kaggle.com/tartakovsky/pytorch-lightning-lstm-timeseries-clean-code
* https://keras.io/examples/timeseries/timeseries_weather_forecasting/
"""

! pip install pytorch-lightning

# Commented out IPython magic to ensure Python compatibility.
# Re-loads all imports every time the cell is ran. 
# %load_ext autoreload
# %autoreload 2

from time import time

import numpy as np
import pandas as pd
pd.options.display.float_format = '{:,.5f}'.format
from IPython.display import display

# Sklearn tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Neural Networks
import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader

import pytorch_lightning as pl
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.loggers.csv_logs import CSVLogger

# Plotting
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib as mpl
plt.style.use('bmh')
mpl.rcParams['figure.figsize'] = 18, 8

! wget -nc "https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip"

"""Так выглядит датасет"""

from zipfile import ZipFile
zip_file = ZipFile("jena_climate_2009_2016.csv.zip")
zip_file.extractall()
csv_path = "jena_climate_2009_2016.csv"

data = pd.read_csv(csv_path)
data = data.iloc[::200] # only each 200th point

data.head()

date_time_key = "Date Time"
data[date_time_key] = pd.to_datetime(data[date_time_key], infer_datetime_format=True)
data.set_index(date_time_key, inplace=True)
data.sort_index(inplace=True)
data.head()

data.shape

"""# Визуализация"""

titles = [
    "Pressure",
    "Temperature",
    "Temperature in Kelvin",
    "Temperature (dew point)",
    "Relative Humidity",
    "Saturation vapor pressure",
    "Vapor pressure",
    "Vapor pressure deficit",
    "Specific humidity",
    "Water vapor concentration",
    "Airtight",
    "Wind speed",
    "Maximum wind speed",
    "Wind direction in degrees",
]

feature_keys = [
    "p (mbar)",
    "T (degC)",
    "Tpot (K)",
    "Tdew (degC)",
    "rh (%)",
    "VPmax (mbar)",
    "VPact (mbar)",
    "VPdef (mbar)",
    "sh (g/kg)",
    "H2OC (mmol/mol)",
    "rho (g/m**3)",
    "wv (m/s)",
    "max. wv (m/s)",
    "wd (deg)",
]

colors = [
    "blue",
    "orange",
    "green",
    "red",
    "purple",
    "brown",
    "pink",
    "gray",
    "olive",
    "cyan",
]



def show_raw_visualization(data):
    time_data = data.index
    fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor="w", edgecolor="k")
    for i in range(len(feature_keys)):
        key = feature_keys[i]
        c = colors[i % (len(colors))]
        t_data = data[key]
        t_data.index = time_data
        t_data.head()
        ax = t_data.plot(ax=axes[i // 2, i % 2], color=c, title="{} - {}".format(titles[i], key), rot=25,)
        ax.legend([titles[i]])
    plt.tight_layout()


show_raw_visualization(data)

data_name = "T (degC)"

"""1 Модель"""

from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(data[data_name], model='additive', extrapolate_trend='freq', period=365)
decomposition.plot()
plt.show()

"""2 Модель"""

from pandas.plotting import lag_plot

import pandas as pd
from pandas.plotting import autocorrelation_plot

autocorrelation_plot(data[data_name])
plt.title(f'{data_name} autocorrelation.')
plt.show()

print('autocorrelation coef:', data[data_name].autocorr())

X = data[[data_name]].values

ss = StandardScaler()
X = ss.fit_transform(X)

plt.plot(X)
plt.title(f'Preprocessed "{data_name}" data.')

"""Модифицируем разные регрессии"""

def AR_matrices(Y, k, m):
    X_AR = []
    Y_AR = []
    for i in range(len(Y)):
        
        if i < k-1: continue
        if i+m >= len(Y): break
        
        ax_ar = Y[i+1-k:i+1].reshape(-1, )
        X_AR.append(ax_ar)

        ay_ar = Y[i+m]#[0]
        Y_AR.append(ay_ar)

    return np.array(X_AR), np.array(Y_AR)

# prepare X and Y matrices
k = 1
X_AR, Y_AR = AR_matrices(X, k=k, m=1)
print('X shape:', X.shape)
print('X_AR shape: ', X_AR.shape)
print('Y_AR shape: ', Y_AR.shape)

N = len(X)//2

X_AR_train, X_AR_test = X_AR[:N], X_AR[N:]
Y_AR_train, Y_AR_test = Y_AR[:N], Y_AR[N:]

"""Модель 3,4,5"""

from sklearn.linear_model import LinearRegression

model = LinearRegression().fit(X_AR_train, Y_AR_train)

Y_pred_test = model.predict(X_AR_test)

plt.figure(figsize=(12, 5))
plt.plot(Y_AR_test, label='True', alpha=1.)
plt.plot(Y_pred_test, label='Prediction', alpha=1.)
plt.xticks(size=14)
plt.yticks(size=14)
plt.legend(loc='best', fontsize=14)
plt.grid()
plt.show()

"""Оценка метрики качества"""

from sklearn.metrics import mean_absolute_error as mse
print('MSE loss:', mse(Y_AR_test, Y_pred_test))

new_k = 30

# prepare X and Y matrices
X_AR_new_k, Y_AR_new_k = AR_matrices(X, k=new_k, m=1)

assert X_AR_new_k.shape == (2073, 30)
assert Y_AR_new_k.shape == (2073, 1)

print('X shape:', X.shape)
print('X_AR shape: ', X_AR_new_k.shape)
print('Y_AR shape: ', Y_AR_new_k.shape)

X_AR_train_new_k, X_AR_test_new_k = X_AR_new_k[:N], X_AR_new_k[N:]
Y_AR_train_new_k, Y_AR_test_new_k = Y_AR_new_k[:N], Y_AR_new_k[N:]

assert X_AR_train_new_k.shape == (1051, 30)
assert X_AR_test_new_k.shape == (1022, 30)
assert Y_AR_train_new_k.shape == (1051, 1)
assert Y_AR_test_new_k.shape == (1022, 1)

model = LinearRegression().fit(X_AR_train_new_k, Y_AR_train_new_k)

Y_pred_test_new_k = model.predict(X_AR_test_new_k)

assert Y_pred_test_new_k.shape == (1022, 1)

for i in range(Y_AR_test.shape[1]):
    plt.figure(figsize=(12, 5))
    plt.plot(Y_AR_test_new_k[:, i], label='True', alpha=1.)
    plt.plot(Y_pred_test_new_k[:, i], label=f'Prediction, k={new_k}', alpha=.7, color='C4')
    plt.plot(Y_pred_test[-Y_AR_test_new_k.shape[0]:], label='Prediction, k=1', alpha=1.)
    plt.xticks(size=14)
    plt.yticks(size=14)
    plt.legend(loc='best', fontsize=14)
    plt.grid()
    plt.show()

"""Модели - LSTM"""

class TimeseriesDataset(Dataset):   
    '''
    Custom Dataset subclass. 
    Serves as input to DataLoader to transform X 
      into sequence data using rolling window. 
    DataLoader using this dataset will output batches 
      of `(batch_size, seq_len, n_features)` shape.
    Suitable as an input to RNNs. 
    '''
    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 1):
        self.X = torch.tensor(X).float()
        self.y = torch.tensor(y).float()
        self.seq_len = seq_len

    def __len__(self):
        return self.X.__len__() - (self.seq_len-1)

    def __getitem__(self, index):
        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])

class DataModule(pl.LightningDataModule):
    '''
    LightningDataModule:
    https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html
    '''
    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):
        super().__init__()
        self.seq_len = seq_len
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.X_train = None
        self.y_train = None
        self.X_test = None
        self.X_test = None
        self.columns = None
        self.preprocessing = None

    def prepare_data(self):
        pass

    def setup(
        self,
        stage=None, 
        X_train=X_AR_train,
        y_train=Y_AR_train,
        X_test=X_AR_test,
        y_test=Y_AR_test,
    ):
        # Assign train/test datasets for use in dataloaders
        if stage == 'fit' or stage is None:
            self.X_train = X_train
            self.y_train = y_train.reshape((-1, 1))
            self.X_val = X_test
            self.y_val = y_test.reshape((-1, 1))
        if stage == 'test' or stage is None:
            self.X_test = X_test
            self.y_test = y_test.reshape((-1, 1))

    def train_dataloader(self):
        train_dataset = TimeseriesDataset(self.X_train, 
                                          self.y_train, 
                                          seq_len=self.seq_len)
        train_loader = DataLoader(train_dataset, 
                                  batch_size = self.batch_size, 
                                  shuffle = False, 
                                  num_workers = self.num_workers)
        
        return train_loader
    
    def val_dataloader(self):
        val_dataset = TimeseriesDataset(self.X_val, 
                                        self.y_val, 
                                        seq_len=self.seq_len)
        val_loader = DataLoader(val_dataset, 
                                batch_size = self.batch_size, 
                                shuffle = False, 
                                num_workers = self.num_workers)

        return val_loader

    def test_dataloader(self):
        test_dataset = TimeseriesDataset(self.X_test, 
                                         self.y_test, 
                                         seq_len=self.seq_len)
        test_loader = DataLoader(test_dataset, 
                                 batch_size = self.batch_size, 
                                 shuffle = False, 
                                 num_workers = self.num_workers)

        return test_loader

class LSTMRegressor(pl.LightningModule):
    '''
    Standard PyTorch Lightning module:
    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html
    '''
    def __init__(self, 
                 n_features, 
                 hidden_size, 
                 seq_len, 
                 batch_size,
                 num_layers, 
                 dropout, 
                 learning_rate,
                 criterion):
        super(LSTMRegressor, self).__init__()
        self.n_features = n_features
        self.hidden_size = hidden_size
        self.seq_len = seq_len
        self.batch_size = batch_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.criterion = criterion
        self.learning_rate = learning_rate

        self.lstm = nn.LSTM(input_size=n_features, 
                            hidden_size=hidden_size,
                            num_layers=num_layers, # number of LSTM-layers.
                            dropout=dropout, 
                            batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        # lstm_out = (batch_size, seq_len, hidden_size)
        lstm_out, _ = self.lstm(x)
        y_pred = self.linear(lstm_out[:,-1])
        return y_pred
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('train_loss', loss)
        return loss
    
    # since the dataset is rather small, we use test data for validation
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('val_loss', loss)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.criterion(y_hat, y)
        self.log('test_loss', loss)
        return loss

p = dict(
    seq_len = 1,
    batch_size = 70, 
    criterion = nn.MSELoss(),
    max_epochs = 20,
    n_features = 1,
    hidden_size = 100,
    num_layers = 1,
    dropout = 0.2,
    learning_rate = 0.001,
)

model = LSTMRegressor(
    n_features = p['n_features'],
    hidden_size = p['hidden_size'],
    seq_len = p['seq_len'],
    batch_size = p['batch_size'],
    criterion = p['criterion'],
    num_layers = p['num_layers'],
    dropout = p['dropout'],
    learning_rate = p['learning_rate']
)

seed_everything(1)

csv_logger = CSVLogger('./', name='lstm', version='0'),

trainer = Trainer(
    max_epochs=p['max_epochs'],
    accelerator="auto"
)

dm = DataModule(
    seq_len = p['seq_len'],
    batch_size = p['batch_size']
)

trainer.fit(model, dm)
trainer.test(model, datamodule=dm)

"""Визуализация обучения нейронной сети"""

from torch.autograd import Variable 

X_test_lstm = Variable(torch.tensor(X_AR_test, dtype=torch.float).view(-1, 1, 1))
X_test_lstm.shape

Y_pred_test_lstm = model(X_test_lstm).detach().numpy()

plt.figure(figsize=(12, 5))
plt.plot(Y_AR_test, label='True', alpha=1.)
plt.plot(Y_pred_test_lstm, label='Prediction', alpha=1.)
plt.xticks(size=14)
plt.yticks(size=14)
plt.legend(loc='best', fontsize=14)
plt.grid()
plt.show()

print('MSE loss:', mse(Y_AR_test, Y_pred_test_lstm))

seq_len = 20

p = dict(
    seq_len = seq_len,# your code where
    batch_size = 50, 
    criterion = nn.MSELoss(),
    max_epochs = 20,
    n_features = 1,
    hidden_size = 20,
    num_layers = 1,
    dropout = 0.2,
    learning_rate = 0.001,
)

dm = DataModule(
    seq_len = p['seq_len'],
    batch_size = p['batch_size']
)

model = LSTMRegressor(
    n_features = p['n_features'],
    hidden_size = p['hidden_size'],
    seq_len = p['seq_len'],
    batch_size = p['batch_size'],
    criterion = p['criterion'],
    num_layers = p['num_layers'],
    dropout = p['dropout'],
    learning_rate = p['learning_rate']
)

seed_everything(1)

csv_logger = CSVLogger('./', name='lstm', version='0'),

trainer = Trainer(
    max_epochs=p['max_epochs'], accelerator="auto"
)

trainer.fit(model, dm)
trainer.test(model, datamodule=dm)

X_AR_test_new_k_lstm = Variable(torch.tensor(X_AR_test_new_k, dtype=torch.float)[:, :, None]) 
assert X_AR_test_new_k_lstm.shape == torch.Size([1022, 30, 1])

Y_pred_test_new_k_lstm = model(X_AR_test_new_k_lstm).detach().numpy()

for i in range(Y_AR_test.shape[1]):
    plt.figure(figsize=(12, 5))
    plt.plot(Y_AR_test, label='True', alpha=1.)
    plt.plot(Y_pred_test_new_k_lstm, label=f'Prediction, k={new_k}', alpha=.7, color='C4')
    plt.plot(Y_pred_test_lstm, label='Prediction, k=1', alpha=1.)
    plt.xticks(size=14)
    plt.yticks(size=14)
    plt.legend(loc='best', fontsize=14)
    plt.grid()
    plt.show()

print('MSE loss k=1:', mse(Y_pred_test_lstm, Y_AR_test))
print(f'MSE loss k={seq_len}:', mse(Y_pred_test_new_k_lstm, Y_AR_test_new_k))

"""# Вывод

Лучше всего результат показывает реккурентная сеть LSTM
"""